{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-mkdocs","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"lessons/01_from_scratch/","title":"Data Engineering from Scratch","text":""},{"location":"lessons/01_from_scratch/#01-data-pipelines-from-scratch","title":"01 Data Pipelines From Scratch","text":"<p>\u201cWithout a systematic way to start and keep data clean, bad data will happen.\u201d ~ Donato Diorio</p> <p> Source: draw.io</p>"},{"location":"lessons/01_from_scratch/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Learning Outcomes</li> <li>Data</li> <li>Tools</li> <li>Data Pipelines</li> <li>Building a Framework<ul> <li>Set Up Dev Environment</li> <li>Extract</li> <li>Transform</li> <li>Load</li> </ul> </li> <li>Reproducible Pipelines</li> <li>Scheduling</li> <li>Exercises</li> <li>Resources</li> </ol>"},{"location":"lessons/01_from_scratch/#1-overview","title":"1. Overview","text":"<p>In order to create successful and reproducible data pipelines we need -- at the bare minimum -- tools that allow us manage where and how we store our data, how we run our computations, and how we version control everything we do. This is what we will focus on in this part of the workshop.</p> <p>The assumption of this section is that most of your data work can fit in a computer but, if the need were to arise, you could still use the code in this section with a beefier machine and it would get the job done without a problem.</p> <p>Since the files we'll interact with will most likely live in a remote server, we'll  1. extract a copy of the data we'll use and save it to our local files; 2. transform the data into the shape and form we need it to be in; 3. load it into a local data warehouse based on the popular tool, duckdb; 4. version our code and data using dvc and git; 5. create a command line tool to run all of our jobs; 6. create a reproducible pipeline that can capture different workflows.</p> <p>Before we get started, let's go over the learning outcomes for today. :)</p> <p>Note: While one of the main components of data orchestration is scheduling, you can sill create pipelines and reproducible workflows that can be triggered manually rather than via a schedule.</p>"},{"location":"lessons/01_from_scratch/#2-learning-outcomes","title":"2. Learning Outcomes","text":"<p>Before we get started, let's go over the learning outcomes for this section of the workshop.</p> <p>By the end of this lesson you will be able to, 1. Discuss what ETL and ELT Pipelines are. 2. Understand how to read and combine data that comes from different sources. 3. Create data pipelines using open-source tools. 4. Develop command line tools.</p>"},{"location":"lessons/01_from_scratch/#3-data","title":"3. Data","text":"<p>All three data files contain similar information about how many bicycles have been rented each hour, day, week and months for several years and for each city government we are working with.</p> <p>You can get more information about the data of each city using the following links.</p> <ul> <li>Seoul, South Korea</li> <li>London, England, UK</li> <li>Washington, DC, USA</li> <li>Porto, Portugal -- This one was shared in Kaggle, but you can also find the original source with more, up-to-date data, here.</li> </ul> <p>Note: Some datasets might come with another file containing daily information, but for our purposes, we will be using the hourly one.</p> <p>Here are the variables that appear in all data sets.</p> London Seoul Washington Porto date Date instant instant count Rented Bike Count date dteday temperature Hour seasons season t2 Temperature(C) year yr humidity Humidity(%) month mnth wind_speed Wind speed (m/s) hour hr weather_code Visibility  (10m) is_holiday holiday is_holiday Dew point temperature(\u00ef\u00bf\u00bdC) weekday weekday is_weekend Solar Radiation (MJ/m2) workingday workingday seasons Rainfall(mm) weathersit weathersit Snowfall(cm) temperature temp Seasons count atemp Holiday humidity hum Functioning Day wind_speed windspeed casual casual registered registered cnt <p>Since all of these datasets were generated with different logic (e.g. celsius vs fahrenheit, or other divergent measures) and, most likely, by different systems, we can expect more inconsistencies than just unmatching column names, numerical formats, and data collected.</p> <p>We will walk through an example pipeline and several cleaning steps after we discuss the tools we will be using today.</p>"},{"location":"lessons/01_from_scratch/#4-tools","title":"4. Tools","text":"<p>The tools that we will use in this section of the workshop are the following.</p> <ul> <li>pandas - \"is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\"</li> <li>dvc - \"DVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code.\"</li> <li>ibis - \"Ibis is a Python library that provides a lightweight, universal interface for data wrangling. It helps Python users explore and transform data of any size, stored anywhere.\"</li> <li>typer - \"Typer is a library for building CLI applications that users will love using and developers will love creating. Based on Python 3.6+ type hints.\"</li> <li>pathlib - allows us to manipulate paths as if they were python objects.</li> </ul> <p>Let's get started building data pipelines! :)</p>"},{"location":"lessons/01_from_scratch/#5-data-pipelines","title":"5. Data Pipelines","text":"<p> Source: Striim</p> <p>There are different kinds of data pipelines, but two, in particular, dominate a big part of the data engineering world today, ETL and ELT pipelines.</p> <p>What are ETL Pipelines?</p> <p>The acronym ETL stands for Extract, Transform, and Load, and it is the process where data gets extracted from one or multiple sources, it gets processed in-transit, and then it gets loaded into place where data consumers can use (e.g. a data warehouse). These consumers can be data analysts, data scientists, and machine learning engineers, among many others.</p> <p>What are ELT Pipelines? With this approach, all the data, structured and unstructured, gets loaded into a data lake or warehouse before it gets transformed. With this approach, a lot of money spent on compute can be saved by only processing the data we need rather than all of it.</p> <p>What are Reverse ETL Pipelines? Reverse ETL tools take data from the data lake or warehouse back into business (critical) applications. For example, information about new customers that have not yet been populated into salesforce or other marketing tools for further use by the marketing, sales, and finance teams, and so on...</p> <p>Why should you learn how to create them?</p> <p>Data Pipeline tools enable data integration strategies by allowing companies to gather data from multiple data sources and consolidate it into a single, centralized location. ETL tools also make it possible for different types of data to work together, for example, data generated by the company can be combined with GPS and Temperature data coming from different sources.</p> <p>As data professionals, our task is to create value for our organizations, our clients and our collaborators using some of or all the data at our disposal. However, there are factors that can delay this process a little bit or a lot, for example, we often need to understand beforehand, 1. Information about the process by which the data we're dealing with was generated, e.g.     - Point of sale     - Clicks on an online marketplace like Amazon, Etzy, Ebay, ect.     - A/B Test Results     - ... 2. Information about the transformations that occurred during the cleaning and merging process, prior to us jumping on board,     - Celsius degrees were converted into fahrenheit     - Prices in Chilean pesos were converted to Rands     - Non-numerical and unavailable observations now contain \"Not Available\" or \"Unknown\"     - ... 3. Information about how the data was stored and where. For instance,     - Parquet format     - NOSQL or SQL database     - CSV     - ...</p> <p>Understanding how the three processes described above flow will help us have more knowledge about the data that we are going to use, and how to best access it, transform it, and model it before we put it to good use.</p> <p>Let's walk through an example of a data pipeline using data from wildfires between 1983-2020 in the United States. You can find more information about the dataset here.</p> <pre><code>import pandas as pd\nfrom pathlib import Path\n</code></pre> <pre><code>!pwd\n</code></pre> <pre><code>/home/ramonperez/Tresors/datascience/tutorials/ph_pycon23/notebooks\n</code></pre> <pre><code>data_path = Path().cwd().parent/\"data\"\ndata_path\n</code></pre> <pre><code>PosixPath('/home/ramonperez/Tresors/datascience/tutorials/ph_pycon23/data')\n</code></pre> <pre><code>example_data_in = data_path.joinpath(\"example\", \"federal_firefighting_costs.csv\")\npd.read_csv(example_data_in).head()\n</code></pre> Year Fires Acres ForestService DOIAgencies Total 0 1985 82,591 2,896,147 $161,505,000 $78,438,000 $239,943,000 1 1986 85,907 2,719,162 $111,625,000 $91,153,000 $202,778,000 2 1987 71,300 2,447,296 $253,657,000 $81,452,000 $335,109,000 3 1988 72,750 5,009,290 $429,609,000 $149,317,000 $578,926,000 4 1989 48,949 1,827,310 $331,672,000 $168,115,000 $499,787,000 <pre><code>pd.read_csv(example_data_in).dtypes\n</code></pre> <pre><code>Year              int64\nFires            object\nAcres            object\nForestService    object\nDOIAgencies      object\nTotal            object\ndtype: object\n</code></pre> <p>As you can see, most columns contain a <code>$</code> dollar sign and some <code>,</code> commas, and because this forces Python to treat numbers as objects (or strings) rather than <code>int</code>'s or <code>float</code>'s, we will have to remove these signs in our transformation step after extracting the data and before loading a clean version of it to a new location. Let's create 3 re-usable functions.</p> <pre><code>def extract(path):\n    return pd.read_csv(path)\n</code></pre> <p>As you saw above, only the last 5 variables have commas (<code>,</code>) and dollar symbols (<code>$</code>) so we will replace both with an empty space (<code>\"\"</code>) using a <code>for</code> loop.</p> <pre><code>def transform(data):\n    for col in data.iloc[:, 1:].columns:\n        data[col] = data[col].str.replace(r'[^0-9]+', '', regex=True).astype(int)\n    return data\n</code></pre> <p>For the \"load\" process, we will save the data as a <code>parquet</code> file. This is one of the most popular formats to save data in due to its compression capabilities, orientation, and speed gains in analytical workloads.</p> <p>Here's an example on the differences between the row-like format and the columnar format of parquet files. If this interests you you can read more about it here</p> <p> Source: Scylla DB</p> <pre><code>def load(data, path):\n    data.to_parquet(path)\n    print(\"Successfully Loaded Your Modified Data!\")\n</code></pre> <p>Let's create an output path and with a file name to save our data as.</p> <pre><code>data_path\n</code></pre> <pre><code>PosixPath('/home/ramonperez/Tresors/datascience/tutorials/ph_pycon23/data')\n</code></pre> <pre><code>example_data_out = data_path.joinpath(\"example\", \"my_test.parquet\")\n</code></pre> <p>When we have all the steps ready, we create a new function containing our graph using the <code>flow</code> decorator. We can give this function a name, for example, <code>\"Example Pipeline! \ud83d\ude0e\"</code> and then chain the tasks we created previously in the order in which they should be run.</p> <pre><code>def example_etl(example_data_in, example_data_out):\n    data = extract(example_data_in)\n    data_clean = transform(data)\n    load(data_clean, example_data_out)\n    print(\"Your Pipeline Ran Successfully!\")\n</code></pre> <p>We are ready to run our workflow.</p> <pre><code>example_etl(example_data_in, example_data_out)\n</code></pre> <pre><code>Successfully Loaded Your Modified Data!\nYour Pipeline Ran Successfully!\n</code></pre> <p>To make sure we did everything correctly, let's create a quick visualization with pandas.</p> <pre><code>pd.read_parquet(example_data_out).plot(\n    x='Year',\n    y=\"ForestService\", \n    kind='line',\n    title=\"Forest Service costs by year\"\n);\n</code></pre> <p></p>"},{"location":"lessons/01_from_scratch/#6-building-a-framework","title":"6. Building a Framework","text":"<p>In order to do our data engineering work with a personalized framework, there are a few strategies we would take. - We could keep the code in our computers and copy it to a new project whenever we need to create new pipelines. - We could create a package and either upload it to PyPI or Gemfury (a private repository of packages for different programming languages). - We could keep in neatly organized in GitHub and clone the repository to every new project.</p> <p>For this use case, it would be great to work with a library that we could continuously update rather than notebooks and files getting copied around. This will help us stay organized and manage dependencies more effectively.</p> <p>Let's get started. :)</p>"},{"location":"lessons/01_from_scratch/#61-setting-up-a-dev-environment","title":"6.1 Setting Up a Dev Environment","text":"<p>Building frameworks can be super straightforward or a slightly cumbersome project. Because of this and because we are already using an environment to run this workshop in, I will leave here what I think is an excellent resource for learning about how to create Python Packages.</p> <pre><code>from IPython.display import HTML\nHTML(\"\"\"\n&lt;div align=\"center\"&gt;\n    &lt;iframe width=\"700\" height=\"450\"\n    src=\"https://youtube.com/embed/l7zS8Ld4_iA\"\n    &lt;/iframe&gt;\n&lt;/div&gt;\n\"\"\")\n</code></pre>"},{"location":"lessons/01_from_scratch/#62-extract","title":"6.2 Extract","text":"<p>Extracting data might seem straightforward, but it can come with plenty of caveats. For example, to access sensitive data you might need not only credentials but also access to VPNs (virtual private networks), you might have similar data in different kinds of formats (customer tables stored in legacy databases and new ones), or you might have different data all stored in one place (images, tables, and text all in a datalake) -- if you're lucky.</p> <p>To tackle these challenges, (1) access and (2) distribution of data, companies such as Airbyte, Fivetran, and others, have come up with solutions that do all the heavy lifting for us. They have created tools that either give you connectors to common data sources (such as S3, BigQuery, and others), or give you an API so that you can develop a custom connector.</p> <p>That said, what we'll do in this section is to create functions that allow us to extract data in different formats or download a dataset from the web. We will create one for each of the examples that we have for the workshop today. Let's get started loading some data.</p> <pre><code>import urllib.request\n</code></pre> <pre><code>def get_and_load_file(url, path_out, file_name):\n    path = Path(path_out)\n    if not path.exists(): path.mkdir(parents=True)\n    urllib.request.urlretrieve(url, path.joinpath(file_name))\n</code></pre> <pre><code>get_and_load_file(\n    url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv\",\n    path_out=\"../data/example\",\n    file_name=\"seoul_exp.csv\"\n)\n</code></pre> <p>The function above will download the file into the <code>\"data/example\"</code> directory. If we do not specify a path for the file to <code>urllib.request.urlretrieve</code>, it would download the file into a temporary directory since it wouldn't know what to do with it or where to put it.</p> <p>A popular alternative to <code>urllib</code> is <code>wget</code>, so you can switch the tools in this function easily. The former is part of the Python Standard Library (so you'll never have to install it), and the latter can be installed with <code>pip</code> or <code>conda</code>.</p> <p>Let's create two more, one for csv files and the other for parquet files.</p> <pre><code>def extract_from_csv(path_in, encoding=None):\n    return pd.read_csv(path_in, encoding=encoding)\n</code></pre> <pre><code>seoul_df = extract_from_csv(\n    path_in=data_path.joinpath(\"seoul\", \"raw\", \"SeoulBikeData.csv\")\n)\nseoul_df.head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  Date Rented Bike Count Hour Temperature(\ufffdC) Humidity(%) Wind speed (m/s) Visibility (10m) Dew point temperature(\ufffdC) Solar Radiation (MJ/m2) Rainfall(mm) Snowfall (cm) Seasons Holiday Functioning Day 0 01/12/2017 254 0 -5.2 37 2.2 2000 -17.6 0.0 0.0 0.0 Winter No Holiday Yes 1 01/12/2017 204 1 -5.5 38 0.8 2000 -17.6 0.0 0.0 0.0 Winter No Holiday Yes 2 01/12/2017 173 2 -6.0 39 1.0 2000 -17.7 0.0 0.0 0.0 Winter No Holiday Yes 3 01/12/2017 107 3 -6.2 40 0.9 2000 -17.6 0.0 0.0 0.0 Winter No Holiday Yes 4 01/12/2017 78 4 -6.0 36 2.3 2000 -18.6 0.0 0.0 0.0 Winter No Holiday Yes <p>The reason, we've added <code>enconing</code> to our function is that files are not always shared in <code>utf-8</code> abd it is important to account for this discrepancy when creating an ETL framework. In fact, one of our datasets has a tricky format itself.</p> <pre><code>{1: \"value\", 1: \"value\", 1: \"value\", 1: \"value\", }\n</code></pre> <pre><code>def extract_from_parquet(path_in, **kwargs):\n    return pd.read_parquet(path_in, **kwargs)\n</code></pre> <pre><code>porto_df = extract_from_parquet(\n    path_in=data_path.joinpath(\"porto\", \"bike_sharing_hourly.parquet\")\n)\nporto_df.head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  instant dteday season yr mnth hr holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt 0 1 2011-01-01 1 0 1 0 0 6 0 1 0.24 0.2879 0.81 0.0 3 13 16 1 2 2011-01-01 1 0 1 1 0 6 0 1 0.22 0.2727 0.80 0.0 8 32 40 2 3 2011-01-01 1 0 1 2 0 6 0 1 0.22 0.2727 0.80 0.0 5 27 32 3 4 2011-01-01 1 0 1 3 0 6 0 1 0.24 0.2879 0.75 0.0 3 10 13 4 5 2011-01-01 1 0 1 4 0 6 0 1 0.24 0.2879 0.75 0.0 0 1 1 <p>One of our files is stored in a SQLite database so we'll use the <code>sqlite3</code> module, which is part of of the Python Standard Library, to read it.</p> <pre><code>import sqlite3\n</code></pre> <pre><code>def extract_from_db(path_in, query):\n    conn = sqlite3.connect(path_in)\n    return pd.read_sql_query(query, conn)\n</code></pre> <pre><code>london_df = extract_from_db(\n    path_in=data_path.joinpath(\"london\", \"london_bikes.db\"),\n    query=\"SELECT * FROM uk_bikes\"\n)\nlondon_df.head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  timestamp cnt t1 t2 hum wind_speed weather_code is_holiday is_weekend seasonreal 0 2015-01-04 00:00:00 182 3.0 2.0 93.0 6.0 3.0 0.0 1.0 3.0 1 2015-01-04 01:00:00 138 3.0 2.5 93.0 5.0 1.0 0.0 1.0 3.0 2 2015-01-04 02:00:00 134 2.5 2.5 96.5 0.0 1.0 0.0 1.0 3.0 3 2015-01-04 03:00:00 72 2.0 2.0 100.0 0.0 1.0 0.0 1.0 3.0 4 2015-01-04 04:00:00 47 2.0 0.0 93.0 6.5 1.0 0.0 1.0 3.0 <p>Lastly, JSON files can be tricky to handle so we'll try two quick cases here using a try-except approach, but take note that as your projects evolve, it is highly likely that this function might change. A very good tool to handle large amounts of unstructured JSON that can later be formatted as a dataframe (or many), is Dask Bags.</p> <pre><code>def extract_from_json(path_in, **kwargs):\n    try:\n        data =  pd.read_json(path_in, kwargs)\n    except:\n        with open(path_in, 'r') as f:\n            data = json.loads(f.read())\n            data = pd.json_normalize(data, kwargs)\n    return data\n</code></pre> <pre><code>dc_df = extract_from_json(\n    path_in=data_path.joinpath(\"wash_dc\", \"washington.json\")\n)\ndc_df.head()\n</code></pre> <pre><code>/tmp/ipykernel_89543/2415585750.py:3: FutureWarning: Starting with pandas version 2.0 all arguments of read_json except for the argument 'path_or_buf' will be keyword-only.\n  data =  pd.read_json(path_in, kwargs)\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  instant dteday season yr mnth hr holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt 0 1 2011-01-01 1 0 1 0 0 6 0 1 0.24 0.2879 0.81 0.0 3 13 16 1 2 2011-01-01 1 0 1 1 0 6 0 1 0.22 0.2727 0.80 0.0 8 32 40 2 3 2011-01-01 1 0 1 2 0 6 0 1 0.22 0.2727 0.80 0.0 5 27 32 3 4 2011-01-01 1 0 1 3 0 6 0 1 0.24 0.2879 0.75 0.0 3 10 13 4 5 2011-01-01 1 0 1 4 0 6 0 1 0.24 0.2879 0.75 0.0 0 1 1 <p>Now that we have our functions, we want to create a file and attach to it the minimum functionality possible to use it as a command line tool. We will do this with the popular package called <code>typer</code>. It offers a delightful user experience, it respects (and annoys you at times) by type-checking your code, and it provides you with beautifully-formatted output based on the rich python library.</p> <p>With <code>typer</code> we can create CLIs in several ways, and, for our purposes, we will pick the most straightforward one which works by adding <code>typer.run()</code> around a main function that encapsulates your application logic, in our case, our extract function and the upcoming ones.</p> <pre><code>%%writefile ../src/data_eng/extract.py\n\nimport urllib.request\nfrom pathlib import Path\nimport pandas as pd\nimport json\nimport typer\nfrom typing import Optional\nfrom load import save_data\n\n\ndef get_and_load_file(url, path_out, file_name):\n    path = Path(path_out)\n    if not path.exists():\n        path.mkdir(parents=True)\n    urllib.request.urlretrieve(url, path.joinpath(file_name))\n\ndef extract_from_csv(path_in, encoding=None):\n    return pd.read_csv(path_in, encoding=encoding)\n\ndef extract_from_db(path_in, query):\n    conn = sqlite3.connect(path_in)\n    return pd.read_sql_query(query, conn)\n\ndef extract_from_parquet(path_in, **kwargs):\n    return pd.read_parquet(path_in, **kwargs)\n\ndef extract_from_json(path_in, **kwargs):\n    try:\n        data =  pd.read_json(path_in, **kwargs)\n    except:\n        with open(path_in, 'r') as f:\n            data = json.loads(f.read())\n            data = pd.json_normalize(data, kwargs)\n    return data\n\ndef main(\n    arg: str = typer.Option(...),\n    url: Optional[str] = typer.Option(None),\n    path_in: Optional[str] = typer.Option(None),\n    path_out: Optional[str] = typer.Option(None),\n    encoding: Optional[str] = typer.Option(None),\n    file_name: Optional[str] = typer.Option(None),\n    query: Optional[str] = typer.Option(None),\n):\n    if arg == \"get\":\n        get_and_load_file(url, path_out, file_name)\n    elif arg == \"csv\":\n        data = extract_from_csv(path_in, encoding=encoding)\n        save_data(data, path_out, file_name)\n    elif arg == \"pq\":\n        data = extract_from_parquet(path_in)\n        save_data(data, path_out, file_name)\n    elif arg == \"json\":\n        data = extract_from_json(path_in)\n        save_data(data, path_out, file_name)\n    elif arg == \"db\":\n        data = extract_from_db(path_in, query)\n        save_data(data, path_out, file_name)\n    else:\n        print(f\"Could not understand argument {arg}. Please use 'pq' for parquet files, 'db' for database, json, or csv in lowercase.\")\n\n    print(\"Data Extracted Successfully!\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre> <pre><code>Overwriting ../src/data_eng/extract.py\n</code></pre> <p>Note that we added different parameters to our <code>main</code> function to control the behavior of our CLI app. - <code>arg</code> - the kind of data we're extracting. - <code>url</code> - the url for when we need to download files from somewhere - <code>path_in</code> - where is the data at - <code>path_out</code> - where is that data going to - <code>encoding</code> - what kind of enconding are we reading the file with - <code>file_name</code> - what is going to be the new name of the output file - <code>query</code> - query for the data we want from the SQL database</p> <p>A few important things to note: - parameters containing underscores <code>_</code> will be switched into a dash <code>-</code> by <code>typer</code>, so <code>file_name</code> will be <code>file-name</code> - commands are run with two dashes and with a space in between it and the argument being passes, e.g. <code>--name PyCon</code> - <code>typer.Option(None)</code> indicates to <code>typer</code> that this command can be optional, hence, when we change the kind of file we're extracting we can by pass having to put an option on others. - by adding <code>Optional</code> around the type of a function parameter, you are also letting typer that that parameter is optional and that its default argument is <code>None</code>. - a parameter with only a type is a required parameter.</p> <pre><code>pd.read_parquet(\"../data/seoul/raw/testing_cli_extract2.csv\").head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  Date Rented Bike Count Hour Temperature(\ufffdC) Humidity(%) Wind speed (m/s) Visibility (10m) Dew point temperature(\ufffdC) Solar Radiation (MJ/m2) Rainfall(mm) Snowfall (cm) Seasons Holiday Functioning Day 0 01/12/2017 254 0 -5.2 37 2.2 2000 -17.6 0.0 0.0 0.0 Winter No Holiday Yes 1 01/12/2017 204 1 -5.5 38 0.8 2000 -17.6 0.0 0.0 0.0 Winter No Holiday Yes 2 01/12/2017 173 2 -6.0 39 1.0 2000 -17.7 0.0 0.0 0.0 Winter No Holiday Yes 3 01/12/2017 107 3 -6.2 40 0.9 2000 -17.6 0.0 0.0 0.0 Winter No Holiday Yes 4 01/12/2017 78 4 -6.0 36 2.3 2000 -18.6 0.0 0.0 0.0 Winter No Holiday Yes"},{"location":"lessons/01_from_scratch/#63-transform","title":"6.3 Transform","text":"<p>Depending on tasks that await the final output files or tables of an ETL pipeline (e.g. reporting metrics, building machine learning models, forecasting, etc.), this step can be very convoluted or slightly straightforward. In general, though, this step can include lots of cleaning and normalization functions, and a schema setter, among many others.</p> <p>The cleaning steps might include dealing with missing values, cleaning emails, names, addresses, and others, or putting values of similar nature into the same format (e.g. different weather measurements into one).</p> <p>Normalization can be anything from changing column names with the same values but different names to have the same one, or height in cm vs feet and inches to be in the same measure, or ..., you get the point. :)</p> <p>A schema is the way in which we represent not only the content and type of our new data files/tables, but also the way in which we represent their relationship. A common schema is the star schema. An uncommon (but rising star) schema is the activity schema.</p> <pre><code>import re\n</code></pre> <pre><code>#create some toy data for our Bicycle problem\n\ntoy_data = pd.DataFrame({\"Postal Codes\": [22345, 32442, 20007], \n                         \"Cities'\":       [\"Miami,  FL\", \"Dallas, TX\", \"Washington, DC\"],\n                         \"Dates as mm-dd-yyy\":         pd.date_range(start='9/27/2021', periods=3)})\ntoy_data\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  Postal Codes Cities' Dates as mm-dd-yyy 0 22345 Miami,  FL 2021-09-27 1 32442 Dallas, TX 2021-09-28 2 20007 Washington, DC 2021-09-29 <pre><code>def clean_col_names(index_of_cols: pd.Index):\n    return [re.sub(r'[^a-zA-Z0-9\\s_]', '', col).lower().replace(r\" \", \"_\") for col in index_of_cols]\n</code></pre> <pre><code>clean_col_names(toy_data)\n</code></pre> <pre><code>['postal_codes', 'cities', 'dates_as_mmddyyy']\n</code></pre> <pre><code>def extract_dates(data, date_col, hour_col=None):\n\n    data[\"date\"] = pd.to_datetime(data[date_col], infer_datetime_format=True)\n    if not data.columns.isin([\"hour\", \"Hour\", \"hr\", \"HR\"]).any():\n        data[\"hour\"] = data['date'].dt.hour\n        #Time series datasets need to be ordered by time.\n        data.sort_values([\"date\", \"hour\"], inplace=True)\n    elif hour_col:\n        data.sort_values([\"date\", hour_col], inplace=True)\n    else:\n        print(\"You must figure out how the hour works in your file.\")\n\n    data[\"year\"]           = data['date'].dt.year\n    data[\"month\"]          = data['date'].dt.month\n    data[\"week\"]           = data['date'].dt.isocalendar().week\n    data[\"day\"]            = data['date'].dt.day\n    data[\"day_of_week\"]    = data['date'].dt.dayofweek\n    data[\"is_month_start\"] = data['date'].dt.is_month_start    \n\n    data.drop('date', axis=1, inplace=True)\n    return data\n</code></pre> <pre><code>extract_dates(london_df, \"timestamp\").head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  timestamp cnt t1 t2 hum wind_speed weather_code is_holiday is_weekend seasonreal hour year month week day day_of_week is_month_start 0 2015-01-04 00:00:00 182 3.0 2.0 93.0 6.0 3.0 0.0 1.0 3.0 0 2015 1 1 4 6 False 1 2015-01-04 01:00:00 138 3.0 2.5 93.0 5.0 1.0 0.0 1.0 3.0 1 2015 1 1 4 6 False 2 2015-01-04 02:00:00 134 2.5 2.5 96.5 0.0 1.0 0.0 1.0 3.0 2 2015 1 1 4 6 False 3 2015-01-04 03:00:00 72 2.0 2.0 100.0 0.0 1.0 0.0 1.0 3.0 3 2015 1 1 4 6 False 4 2015-01-04 04:00:00 47 2.0 0.0 93.0 6.5 1.0 0.0 1.0 3.0 4 2015 1 1 4 6 False <p>A common tasks in data science is to create dummy variables, which is ofter referred to as one-hot encoding. These are binary representations of a category, for example, if you have a columns with different kinds of cars, after you one-hot encode it, you will have one column for sedan, coupe, convertible, and so on, and each will be represented with a 0 or a 1 for when it is available and when it isn't, respectively.</p> <pre><code>def one_hot(data, cols_list, **kwargs):\n    return pd.get_dummies(data=data, columns=cols_list, **kwargs)\n</code></pre> <pre><code>one_hot(seoul_df, [\"Seasons\", \"Holiday\"], drop_first=True).head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  Date Rented Bike Count Hour Temperature(\ufffdC) Humidity(%) Wind speed (m/s) Visibility (10m) Dew point temperature(\ufffdC) Solar Radiation (MJ/m2) Rainfall(mm) Snowfall (cm) Functioning Day Seasons_Spring Seasons_Summer Seasons_Winter Holiday_No Holiday 0 01/12/2017 254 0 -5.2 37 2.2 2000 -17.6 0.0 0.0 0.0 Yes 0 0 1 1 1 01/12/2017 204 1 -5.5 38 0.8 2000 -17.6 0.0 0.0 0.0 Yes 0 0 1 1 2 01/12/2017 173 2 -6.0 39 1.0 2000 -17.7 0.0 0.0 0.0 Yes 0 0 1 1 3 01/12/2017 107 3 -6.2 40 0.9 2000 -17.6 0.0 0.0 0.0 Yes 0 0 1 1 4 01/12/2017 78 4 -6.0 36 2.3 2000 -18.6 0.0 0.0 0.0 Yes 0 0 1 1 <pre><code>def add_location_cols(data, place):\n    data[\"loc_id\"] = place\n    return data\n</code></pre> <pre><code>add_location_cols(dc_df, \"DC\").head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  instant dteday season yr mnth hr holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt loc_id 0 1 2011-01-01 1 0 1 0 0 6 0 1 0.24 0.2879 0.81 0.0 3 13 16 DC 1 2 2011-01-01 1 0 1 1 0 6 0 1 0.22 0.2727 0.80 0.0 8 32 40 DC 2 3 2011-01-01 1 0 1 2 0 6 0 1 0.22 0.2727 0.80 0.0 5 27 32 DC 3 4 2011-01-01 1 0 1 3 0 6 0 1 0.24 0.2879 0.75 0.0 3 10 13 DC 4 5 2011-01-01 1 0 1 4 0 6 0 1 0.24 0.2879 0.75 0.0 0 1 1 DC <pre><code>def fix_and_drop(data, col_to_fix, mapping, cols_to_drop):\n    data[col_to_fix] = data[col_to_fix].map(mapping)\n    return data.drop(cols_to_drop, axis=1)\n</code></pre> <pre><code>seasons_london = {'Spring': 0, 'Summer': 1, 'Fall': 2, 'Winter': 3}\ncols_drop_london = ['t2', 'weather_code']\n</code></pre> <pre><code>london_df.head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  timestamp cnt t1 t2 hum wind_speed weather_code is_holiday is_weekend seasonreal hour year month week day day_of_week is_month_start 0 2015-01-04 00:00:00 182 3.0 2.0 93.0 6.0 3.0 0.0 1.0 3.0 0 2015 1 1 4 6 False 1 2015-01-04 01:00:00 138 3.0 2.5 93.0 5.0 1.0 0.0 1.0 3.0 1 2015 1 1 4 6 False 2 2015-01-04 02:00:00 134 2.5 2.5 96.5 0.0 1.0 0.0 1.0 3.0 2 2015 1 1 4 6 False 3 2015-01-04 03:00:00 72 2.0 2.0 100.0 0.0 1.0 0.0 1.0 3.0 3 2015 1 1 4 6 False 4 2015-01-04 04:00:00 47 2.0 0.0 93.0 6.5 1.0 0.0 1.0 3.0 4 2015 1 1 4 6 False <pre><code>london_df[\"seasonreal\"].map(seasons_london).value_counts()\n</code></pre> <pre><code>0    4394\n1    4387\n3    4330\n2    4303\nName: seasonreal, dtype: int64\n</code></pre> <pre><code>fix_and_drop(london_df, \"seasonreal\", seasons_london, cols_drop_london).head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  timestamp cnt t1 hum wind_speed is_holiday is_weekend seasonreal hour year month week day day_of_week is_month_start 0 2015-01-04 00:00:00 182 3.0 93.0 6.0 0.0 1.0 Winter 0 2015 1 1 4 6 False 1 2015-01-04 01:00:00 138 3.0 93.0 5.0 0.0 1.0 Winter 1 2015 1 1 4 6 False 2 2015-01-04 02:00:00 134 2.5 96.5 0.0 0.0 1.0 Winter 2 2015 1 1 4 6 False 3 2015-01-04 03:00:00 72 2.0 100.0 0.0 0.0 1.0 Winter 3 2015 1 1 4 6 False 4 2015-01-04 04:00:00 47 2.0 93.0 6.5 0.0 1.0 Winter 4 2015 1 1 4 6 False <pre><code>def order_and_merge(data_lists, date=None):\n    pick_order = data_lists[0].columns #takes order of columns of the first dataset\n    #reindexing columns by the order of the first dataset, then sorting by date and hour.\n    if date:\n        new_list = [d.reindex(columns=pick_order).sort_values([date, 'hour']) for d in data_lists]\n    else:\n        new_list = data_lists\n    return pd.concat(new_list) #merge all\n</code></pre>"},{"location":"lessons/01_from_scratch/#exercise","title":"Exercise","text":"<p>Pick any two datasets and - select 5 columns from each - change column names as appropriate - use the order_and_nerge function to combine both</p> <pre><code>london_df.head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  timestamp cnt t1 t2 hum wind_speed weather_code is_holiday is_weekend seasonreal hour year month week day day_of_week is_month_start 0 2015-01-04 00:00:00 182 3.0 2.0 93.0 6.0 3.0 0.0 1.0 Winter 0 2015 1 1 4 6 False 1 2015-01-04 01:00:00 138 3.0 2.5 93.0 5.0 1.0 0.0 1.0 Winter 1 2015 1 1 4 6 False 2 2015-01-04 02:00:00 134 2.5 2.5 96.5 0.0 1.0 0.0 1.0 Winter 2 2015 1 1 4 6 False 3 2015-01-04 03:00:00 72 2.0 2.0 100.0 0.0 1.0 0.0 1.0 Winter 3 2015 1 1 4 6 False 4 2015-01-04 04:00:00 47 2.0 0.0 93.0 6.5 1.0 0.0 1.0 Winter 4 2015 1 1 4 6 False <pre><code>london_df_small = london_df[[\"cnt\", \"t1\", \"wind_speed\", \"seasonreal\", \"hour\"]]\nlondon_df_small.tail()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  cnt t1 wind_speed seasonreal hour 17409 1042 5.0 19.0 Winter 19 17410 541 5.0 21.0 Winter 20 17411 337 5.5 24.0 Winter 21 17412 224 5.5 23.0 Winter 22 17413 139 5.0 22.0 Winter 23 <pre><code>seoul_df.head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  Date Rented Bike Count Hour Temperature(\ufffdC) Humidity(%) Wind speed (m/s) Visibility (10m) Dew point temperature(\ufffdC) Solar Radiation (MJ/m2) Rainfall(mm) Snowfall (cm) Seasons Holiday Functioning Day 0 01/12/2017 254 0 -5.2 37 2.2 2000 -17.6 0.0 0.0 0.0 Winter No Holiday Yes 1 01/12/2017 204 1 -5.5 38 0.8 2000 -17.6 0.0 0.0 0.0 Winter No Holiday Yes 2 01/12/2017 173 2 -6.0 39 1.0 2000 -17.7 0.0 0.0 0.0 Winter No Holiday Yes 3 01/12/2017 107 3 -6.2 40 0.9 2000 -17.6 0.0 0.0 0.0 Winter No Holiday Yes 4 01/12/2017 78 4 -6.0 36 2.3 2000 -18.6 0.0 0.0 0.0 Winter No Holiday Yes <pre><code>seoul_df_small = seoul_df[[\"Rented Bike Count\", \"Temperature(\ufffdC)\", \"Wind speed (m/s)\", \"Seasons\", \"Hour\"]]\nseoul_df_small.head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  Rented Bike Count Temperature(\ufffdC) Wind speed (m/s) Seasons Hour 0 254 -5.2 2.2 Winter 0 1 204 -5.5 0.8 Winter 1 2 173 -6.0 1.0 Winter 2 3 107 -6.2 0.9 Winter 3 4 78 -6.0 2.3 Winter 4 <pre><code>seoul_df_small.columns = london_df_small.columns\nseoul_df_small.columns == london_df_small.columns\n</code></pre> <pre><code>array([ True,  True,  True,  True,  True])\n</code></pre> <pre><code>order_and_merge([seoul_df_small, london_df_small]).shape\n</code></pre> <pre><code>(26174, 5)\n</code></pre> <p>Finally, we take the same approach as with the extract set of functions and wrap our transform workflow into a <code>main</code> function, and then that <code>main</code> function into a typer CLI.</p> <pre><code>%%writefile ../src/data_eng/transform.py\n\nimport pandas as pd, re\nfrom pathlib import Path\nfrom extract import save_data\nimport typer\nfrom typing import Optional, List\n\ndef clean_col_names(index_of_cols: pd.Index):\n    return [re.sub(r'[^a-zA-Z0-9\\s_]', '', col).lower().replace(r\" \", \"_\") for col in index_of_cols]\n\ndef extract_dates(data, date_col, hour_col=None):\n\n    data[\"date\"] = pd.to_datetime(data[date_col], infer_datetime_format=True)\n    if not data.columns.isin([\"hour\", \"Hour\", \"hr\", \"HR\"]).any():\n        data[\"hour\"] = data['date'].dt.hour\n        #Time series datasets need to be ordered by time.\n        data.sort_values([\"date\", \"hour\"], inplace=True)\n    elif hour_col:\n        data.sort_values([\"date\", hour_col], inplace=True)\n    else:\n        print(\"You must figure out how the hour works in your file.\")\n\n    data[\"year\"]           = data['date'].dt.year\n    data[\"month\"]          = data['date'].dt.month\n    data[\"week\"]           = data['date'].dt.isocalendar().week\n    data[\"day\"]            = data['date'].dt.day\n    data[\"day_of_week\"]    = data['date'].dt.dayofweek\n    data[\"is_month_start\"] = data['date'].dt.is_month_start    \n\n    data.drop('date', axis=1, inplace=True)\n    return data\n\ndef one_hot(data, cols_list):\n    return pd.get_dummies(data=data, columns=cols_list)\n\ndef add_location_cols(data, place):\n    data[\"loc_id\"] = place\n    return data\n\ndef fix_and_drop(data, col_to_fix, mapping, cols_to_drop):\n    data[col_to_fix] = data[col_to_fix].map(mapping)\n    return data.drop(cols_to_drop, axis=1)\n\ndef order_and_merge(data_lists):\n    pick_order = data_lists[0].columns #takes order of columns of the first dataset\n    new_list = [d.reindex(columns=pick_order).sort_values(['date', 'hour']) for d in data_lists] #reindexing columns by the order of the first dataset, then sorting by date and hour.\n    return pd.concat(new_list) #merge all\n\ndef main(\n    path_in:   str           = typer.Option(...),\n    date_col:  Optional[str] = typer.Option(None),\n    hour_col:  Optional[str] = typer.Option(None),\n    cols_list: List[str]     = typer.Option(None),\n    place:     Optional[str] = typer.Option(None),\n    path_out:  Optional[str] = typer.Option(None),\n    file_name: Optional[str] = typer.Option(None)\n):\n    data = pd.read_parquet(path_in)\n    # data.columns = clean_col_names(data.columns)\n    data = extract_dates(data, date_col, hour_col)\n    if cols_list:\n        data = one_hot(data, cols_list)\n    data.columns = clean_col_names(data.columns)\n    data = add_location_cols(data, place)\n    save_data(data, path_out, file_name)\n\n    print(\"Data Extracted and Transformed Successfully!\")\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre> <pre><code>Overwriting ../src/data_eng/transform.py\n</code></pre> <pre><code>pd.read_parquet(\"../data/seoul/interim/cli_test_clean.parquet\").head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  date rented_bike_count hour temperaturec humidity wind_speed_ms visibility_10m dew_point_temperaturec solar_radiation_mjm2 rainfallmm ... seasons holiday functioning_day year month week day day_of_week is_month_start loc_id 0 01/12/2017 254 0 -5.2 37 2.2 2000 -17.6 0.0 0.0 ... Winter No Holiday Yes 2017 1 2 12 3 False Seoul 1 01/12/2017 204 1 -5.5 38 0.8 2000 -17.6 0.0 0.0 ... Winter No Holiday Yes 2017 1 2 12 3 False Seoul 2 01/12/2017 173 2 -6.0 39 1.0 2000 -17.7 0.0 0.0 ... Winter No Holiday Yes 2017 1 2 12 3 False Seoul 3 01/12/2017 107 3 -6.2 40 0.9 2000 -17.6 0.0 0.0 ... Winter No Holiday Yes 2017 1 2 12 3 False Seoul 4 01/12/2017 78 4 -6.0 36 2.3 2000 -18.6 0.0 0.0 ... Winter No Holiday Yes 2017 1 2 12 3 False Seoul <p>5 rows \u00d7 21 columns</p>"},{"location":"lessons/01_from_scratch/#62-load","title":"6.2 Load","text":"<p>The loading stage, at least for analytical purposes, tends to be a data warehouse like BigQuery, Redshift, etc., but it can also be a directory in a data lake where files are saved in the highly optimized parquet format.</p> <p>In order for us to simulate a data warehouse locally, we will create a duckdb database using the python library <code>ibis</code>. The reason we will do it this ways is that ibis can translate the schema of our files into the duckdb SQL in one line of code. Effectively saving us boilerplate code that may or may not end up accounting for every feature in our dataframes.</p> <p>Ibis is a very cool project, especially if your SQL skills as basic as mine, so I highly encourage you to check it out. :)</p> <pre><code>import ibis\nfrom pathlib import Path\n</code></pre> <pre><code>data_path = Path().cwd().parent/\"data/\"\n</code></pre> <pre><code>def create_db(path_in, path_out, file_name, table_name):\n    path = Path(path_out)\n    conn = ibis.duckdb.connect(path.joinpath(file_name))\n    conn.register(path_in, table_name=table_name)\n    print(f\"Successfully loaded the {table_name} table!\")\n</code></pre> <pre><code>create_db(\n    path_in=data_path.joinpath(\"porto\", \"bike_sharing_hourly.parquet\"),\n    path_out=\"../data/dwarehouse\",\n    file_name=\"mytest_dw.ddb\",\n    table_name=\"one_day\"\n)\n</code></pre> <pre><code>Successfully loaded the one_day table!\n</code></pre> <p>Let's read in our data to check that it was loaded successfully.</p> <pre><code>import duckdb\n</code></pre> <pre><code>duck = duckdb.connect(\"../data/dwarehouse/mytest_dw.ddb\")\nduck.query(\"SELECT * FROM one_day\").df().head()\n</code></pre>      .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }  instant dteday season yr mnth hr holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt 0 1 2011-01-01 1 0 1 0 0 6 0 1 0.24 0.2879 0.81 0.0 3 13 16 1 2 2011-01-01 1 0 1 1 0 6 0 1 0.22 0.2727 0.80 0.0 8 32 40 2 3 2011-01-01 1 0 1 2 0 6 0 1 0.22 0.2727 0.80 0.0 5 27 32 3 4 2011-01-01 1 0 1 3 0 6 0 1 0.24 0.2879 0.75 0.0 3 10 13 4 5 2011-01-01 1 0 1 4 0 6 0 1 0.24 0.2879 0.75 0.0 0 1 1 <pre><code>def create_parquet(path_in, path_out, file_name):\n    path = Path(path_out)\n    pd.read_parquet(path_in).to_parquet(path.joinpath(file_name))\n    print(f\"Successfully loaded the {file_name} table!\")\n</code></pre> <pre><code>def save_data(data, path_out, file_name):\n    path_out = Path(path_out)\n    if not path_out.exists(): path_out.mkdir(parents=True)\n    data.to_parquet(path_out.joinpath(file_name))\n    print(f\"Successfully loaded the {file_name} table!\")\n</code></pre> <p>We take the same steps as before and wrap our main function with <code>typer.run()</code> to make it a CLI, we test that it works well, and then go on to the next stage.</p> <pre><code>%%writefile ../src/data_eng/load.py\n\nfrom pathlib import Path\nimport ibis\nimport typer\nfrom typing import Optional\nimport pandas as pd\n\n\ndef create_db(path_in, path_out, file_name, table_name):\n    path = Path(path_out)\n    conn = ibis.duckdb.connect(path.joinpath(file_name))\n    conn.register(path_in, table_name=table_name)\n    print(f\"Successfully loaded the {table_name} table!\")\n\ndef create_parquet(path_in, path_out, file_name):\n    path = Path(path_out)\n    pd.read_parquet(path_in).to_parquet(path.joinpath(file_name))\n    print(f\"Successfully loaded the {file_name} table!\")\n\ndef save_data(data, path_out, file_name):\n    path_out = Path(path_out)\n    if not path_out.exists(): path_out.mkdir(parents=True)\n    data.to_parquet(path_out.joinpath(file_name))\n    print(f\"Successfully loaded the {file_name} table!\")\n\n\ndef main(\n    kind: str = typer.Option(...),\n    path_in: Optional[str] = typer.Option(None), \n    path_out: Optional[str] = typer.Option(None),\n    file_name: Optional[str] = typer.Option(None),\n    table_name: Optional[str] = typer.Option(None)\n):\n    if kind == \"db\":\n        create_db(path_in, path_out, file_name, table_name)\n    elif kind == \"pq\":\n        create_parquet(path_in, path_out, file_name)\n    else:\n        print(f\"Could not understand argument {kind}. Please use 'pq' for parquet files or 'db' for database.\")\n\n    print(\"Data Extracted Successfully!\")\n\nif __name__ == \"__main__\":\n    typer.run(main)\n</code></pre> <pre><code>Overwriting ../src/data_eng/load.py\n</code></pre>"},{"location":"lessons/01_from_scratch/#7-reproducible-pipelines","title":"7. Reproducible Pipelines","text":"<p>Data Version Control or <code>dvc</code> is a tool for version almost any kind of file you can think of. From images, text, videos, and excel spreadsheets, to machine learning models and other artifacts. In addition, it is also an excellent tool for tracking machine learning experiments and for creating language agnostic pipelines where you can automatically version control the inputs and outputs of your workflows. </p> <p>If you have ever used git then the following commands will feel like home</p> <ul> <li><code>dvc init</code> --&gt; this the first step to get started using dvc (after installing it of course)</li> <li><code>dvc remote add -d storage gdrive://your_hash</code> --&gt; since we will be tracking files somewhere, this command will help set up a remote repository for these.</li> <li><code>dvc add</code> --&gt; add a file to track.</li> <li><code>dvc push</code> --&gt; push the file to your remote repository.</li> <li><code>dvc pull</code> --&gt; pull the file from your remote repository and into your local machine.</li> <li><code>dvc stage</code> --&gt; allows you to start adding steps to a pipeline. It will create a <code>dvc.yml</code> file containing the steps of the pipeline. This can be modified manually as well.</li> <li><code>dvc repro</code> --&gt; Once we finish our stage, or as we add steps to it, we can run our pipeline with this command. The best part is that the steps will get cashed, so if nothing changes, nothing will get rerun.</li> <li><code>dvc dag</code> --&gt; allows you to visualize your pipelines as a graph in the terminal.</li> </ul> <p>Finally, some of the settings for our repo will be available at <code>.dvc/config</code> and these can be changed manually at any time.</p> <p>Open a terminal in the main directory for this workshop and follow along.</p> <p>To create our pipeline, we'll add the stages step by step.</p> <pre><code>dvc stage add --name seoul_extract \\\n--deps data/seoul/SeoulBikeData.csv \\\n--outs data/seoul/raw/seoul_raw.parquet \\\npython src/data_eng/extract.py --arg csv \\\n--path-in data/seoul/SeoulBikeData.csv \\\n--path-out data/seoul/raw \\\n--file-name seoul_raw.parquet --encoding iso-8859-1\n</code></pre> <pre><code>dvc stage add --name seoul_transform \\\n--deps data/seoul/raw/seoul_raw.parquet \\\n--outs data/seoul/interim/seoul_clean.parquet \\\npython src/data_eng/transform.py \\\n--path-in data/seoul/seoul_raw.parquet \\\n--date-col Date --hour-col Hour --place Seoul \\\n--path-out data/seoul/interim --file-name seoul_clean.parquet\n</code></pre>"},{"location":"lessons/01_from_scratch/#exercise_1","title":"Exercise","text":"<p>Add the load stage into our pipeline following the examples above.</p> <pre><code>dvc stage add --name seoul_load \\\n--deps data/seoul/interim/seoul_clean.parquet \\\n--outs data/dwarehouse/analytics.db \\\npython src/data_eng/load.py --kind db \\\n--path-in data/seoul/interim/seoul_clean.parquet \\\n--path-out data/dwarehouse --name analytics.db \\\n--table-name seoul_main\n</code></pre> <p>Finally we can run our pipeline, evaluate it, and push our files into our remote storage.</p> <pre><code># 1\ndvc repro\n\n# 2\ndvc dag\n\n# 3\ndvc push\n</code></pre> <p>Afterwards, dvc will provide us with git files to track for our project.</p>"},{"location":"lessons/01_from_scratch/#8-scheduling","title":"8. Scheduling","text":"<p>Scheduling our workflows can be super convenient and time-savior when we know we need to run tasks repeatedly. A very common tool for the task is cron, and here is an excellent tutorial to get started with it.</p> <p>Note: Windows users would need to use Windows Subsystem for Linux to use cron or some other tool, potentially in PowerShell.</p> <pre><code>from IPython.display import HTML\nHTML(\"\"\"\n&lt;div align=\"center\"&gt;\n    &lt;iframe width=\"700\" height=\"450\"\n    src=\"https://www.youtube.com/embed/QZJ1drMQz1A\"\n    title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; \n    clipboard-write; encrypted-media; gyroscope; picture-in-picture; \n    web-share\" allowfullscreen&gt;\n    &lt;/iframe&gt;\n&lt;/div&gt;\n\"\"\")\n</code></pre>"},{"location":"lessons/01_from_scratch/#9-exercises","title":"9. Exercises","text":""},{"location":"lessons/01_from_scratch/#exercise-1","title":"Exercise 1","text":"<ol> <li>Open the terminal, create a new directory named <code>dc_bikes</code>, and cd into it.</li> <li>Create two subdirectories, data and src.</li> <li>Create an ETL pipeline with two functions in the transform step.</li> </ol>"},{"location":"lessons/01_from_scratch/#exercise-2","title":"Exercise 2","text":"<ol> <li>Initialize a git and a dvc repository for the project in Exercise 1.</li> <li>Create a new repo in GitHub and commit your initial changes.</li> <li>Create a dvc pipeline with your three stages.</li> <li>Commit your changes.</li> </ol>"},{"location":"lessons/01_from_scratch/#10-resources","title":"10. Resources","text":"<p>If you'd like to expand your knowledge around the tools and concepts that we covered in this lesson, you might find the following list of resources helpful.</p> <ul> <li>To learn more about pandas --&gt; Python for Data Analysis, 3E by Wes McKinney</li> <li>To learn more about ibis --&gt; Ibis + Substrait + DuckDB by Gil Forsyth</li> <li>To learn more about duckdb --&gt; DuckDB Tutorial by Data with Marc</li> <li>To learn more about dvc --&gt; ML Pipeline Decoupled: I managed to Write a Framework Agnostic ML Pipeline with DVC, Rust &amp; Python</li> <li>To learn more about typer --&gt; Data and Machine Learning Model Versioning with DVC by Ruben Winastwan</li> <li>To learn more about cron --&gt; Cron Job: A Comprehensive Guide for Beginners 2023 by Linas L.</li> </ul>"},{"location":"lessons/02_framework/","title":"02 A Framework Approach","text":""},{"location":"lessons/02_framework/#02-a-framework-approach","title":"02 A Framework Approach","text":"<p>\"Automation is not the enemy of jobs. It frees up human beings to do higher-value work.\" ~ Andy Stern.</p>"},{"location":"lessons/02_framework/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Learning Outcomes</li> <li>Tools</li> <li>The (Local) Cloud</li> <li>Classy ETLs</li> <li>Add-ons</li> <li>Data Validation</li> <li>Resources</li> </ol>"},{"location":"lessons/02_framework/#1-overview","title":"1. Overview","text":"<p>In this section, we want to supercharge the minimum requirements we established for ourselves in section 1 (i.e. storage, compute, and version control) while expanding into better portability and orchestration.</p> <p>To accomplish this, we'll give the driver seat to <code>metaflow</code>, a tool originally created at, and open-sourced by, Netflix a few years back.</p> <p>The tool now has a company, Outerbounds, as its main contributor</p>"},{"location":"lessons/02_framework/#2-learning-outcomes","title":"2. Learning Outcomes","text":"<p>By the end of this session you will be able to, - create workflows using metaflow - schedule workflows with different time intervals - understand how to visually inspect workflows</p>"},{"location":"lessons/02_framework/#3-tools","title":"3. Tools","text":"<p>The tools that we will use in this section of the workshop are the following.</p> <ul> <li>localstack --&gt; \"LocalStack is a cloud service emulator that runs in a single container on your laptop or in your CI environment. With LocalStack, you can run your AWS applications or Lambdas entirely on your local machine without connecting to a remote cloud provider!\" ~ localstack</li> <li>Metaflow --&gt; \"Metaflow is a human-friendly Python library that makes it straightforward to develop, deploy, and operate various kinds of data-intensive applications, in particular those involving data science and ML.\" ~ Metaflow docs</li> <li>boto3 --&gt; \"You use the AWS SDK for Python (Boto3) to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services.\" ~ boto3 documentation</li> </ul> <p>Let's get started by evaluating the (local) cloud. :)</p>"},{"location":"lessons/02_framework/#4-the-local-cloud","title":"4. The (Local) Cloud","text":"<p>LocalStack is a tool that allows us to emulate the cloud services provided by AWS in our local machines. Their free tier is a great to get started learning about the cloud and that's why we will be using it here.</p> <p>Note that <code>localstack</code> need docked to be installed in your machine so if this is not available, you won't be able to do a few of the steps below.</p> <p>That said, let's get started.</p> <p>Open up a new terminal start localstack with the following command.</p> <p><pre><code>localstack start\n</code></pre> You should be able to see the following image.</p> <p></p> <p>Now that we have a \"Cloud\" instance running in our machines, let's start by creating a bucket in S3 using boto3.</p> <pre><code>import boto3\n</code></pre> <pre><code>s3_client = boto3.client(\"s3\", endpoint_url=\"http://localhost.localstack.cloud:4566\")\n</code></pre> <p>Note that because we are not actually interacting with a cloud provider like AWS, GCP or Azure, we need to point boto3 towards our local cloud using the parameter <code>endpoint_url</code>.</p> <pre><code>s3_client.create_bucket(Bucket=\"datalake\")\n</code></pre> <pre><code>s3_client.list_buckets()[\"Buckets\"]\n</code></pre> <p>To have less verbose output, and more or less the same functionality, we can create resources instead of clients.</p> <pre><code>s3_resource = boto3.resource(\"s3\", endpoint_url=\"http://localhost.localstack.cloud:4566\")\n</code></pre> <pre><code>s3_resource.create_bucket(Bucket=\"datalake2\")\n</code></pre> <pre><code>for bk in s3_resource.buckets.all():\n    print(bk.name)\n</code></pre> <p>In order to finish setting up localstack for our workshop, we'll need to set up the aws and the metaflow configs.</p> <pre><code>\n</code></pre>"},{"location":"lessons/02_framework/#5-classy-etls","title":"5. Classy ETLs","text":"<p>The way metaflow works is by having the user define classes that inherite the metaflow's <code>FlowSpec</code> class and represent a flow of whatever you'd like to do, e.g., a data pipeline, a dashboard, or training one or many machine learning models, among many other taks.</p> <p>The beauty of metaflow is its simplicity and customizable nature. It's downside is the lack of an easy-to-install and easy-to-set-up user interface where one could visually inspect ones flows.</p> <p>Let's get our hands dirty with our first example.</p> <pre><code>%%writefile ../src/ml_eng/fireflow.py\n\nfrom metaflow import FlowSpec, step\n\nclass FireFlow(FlowSpec):\n\n    @step\n    def start(self):\n        print(\"Hi, this is your first flow!\")\n        from pathlib import Path\n        self.data_path = Path().cwd().parent/\"data\"/\"example\"\n        self.data_in = self.data_path.joinpath(\"federal_firefighting_costs.csv\")\n        self.data_out = self.data_path.joinpath(\"fire_flow_output.parquet\")\n        self.next(self.extract)\n\n    @step\n    def extract(self):\n        import pandas as pd\n        self.data = pd.read_csv(self.data_in)\n        self.next(self.transform)\n\n    @step\n    def transform(self):\n        import pandas as pd\n        for col in self.data.iloc[:, 1:].columns:\n            self.data[col] = self.data[col].str.replace(r'[^0-9]+', '', regex=True).astype(int)\n        self.next(self.load)\n\n    @step\n    def load(self):\n        import pandas as pd\n        self.data.to_parquet(self.data_out)\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print(\"Your first flow finished!\")\n\nif __name__ == \"__main__\":\n    FireFlow()\n</code></pre> <p>Let's run our our file using the following command from our notebook.</p> <p>Note that the same command won't work from the parent directory as the files are referenced from this notebook.</p> <pre><code>!python ../src/ml_eng/fireflow.py run\n</code></pre> <p>Let's go over what just happened.</p> <p>Metaflow keeps track of a lot of things when we run a flow, and the output it just gave us gives us context as to what is happening inside of it. Here's what each piece of it means.</p> <ul> <li><code>2023-02-26 10:57:24.693</code> --&gt; Timestamp for the step</li> <li><code>1677380240122156</code> --&gt; Run ID</li> <li><code>load</code> --&gt; Step Name</li> <li><code>4</code> --&gt; Task ID</li> <li><code>(pid 49081)</code> --&gt; Process ID</li> <li><code>Task is starting.</code> --&gt; Log Message</li> </ul> <p>The way metaflow passes arguments from one step to another and the way it keeps track of everything it touches is via the <code>self</code> argument. Each step is its own encapsulated container running in isolation but with awareness of where to go next after a step finishes.</p>"},{"location":"lessons/02_framework/#exercise","title":"Exercise","text":"<p>Pick one function of each of of the files from the last section (<code>extract.py</code>, <code>transform.py</code>, and <code>load.py</code>) and create a flow with metaflow. Name it, BikesFlow.</p> <p>Now we'll write a proper flow for our earlier pipeline and we'll keep improving it in the next session.</p> <pre><code>%%writefile ../src/ml_eng/data_flow.py\n\nimport urllib.request\nfrom pathlib import Path\nimport pandas as pd, re\n\nfrom metaflow import FlowSpec, step, Parameter\n\nclass MainDataFlow(FlowSpec):\n\n    @step\n    def start(self):\n        import boto3\n        from os.path import join\n        import urllib.request\n\n        url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv'\n        self.bucket = \"datalake\"\n        self.raw_dir = \"raw\"\n        self.raw_data_name = 'SeoulBikeData_test.csv'\n        self.tmp_file, _ = urllib.request.urlretrieve(url, self.raw_data_name)\n\n        s3_resource = boto3.resource(\"s3\", endpoint_url=\"http://localhost.localstack.cloud:4566\")         \n        s3_resource.Object(bucket_name=self.bucket, key=join(self.raw_dir, self.raw_data_name)).upload_file(self.tmp_file)\n\n        self.next(self.extract)\n\n    @step\n    def extract(self):\n        import pandas as pd\n\n        # get the data with its peculiar encoding\n        self.data = pd.read_csv(self.tmp_file, encoding='iso-8859-1', parse_dates=['Date'], infer_datetime_format=True)\n        self.next(self.transform)\n\n\n    @step\n    def transform(self):\n        self.data.columns = [re.sub(r'[^a-zA-Z0-9\\s_]', '', col).lower().replace(r\" \", \"_\") for col in self.data.columns]\n\n        self.data.sort_values(['date', 'hour'], inplace=True)\n\n        self.data[\"year\"]           = self.data['date'].dt.year\n        self.data[\"month\"]          = self.data['date'].dt.month\n        self.data[\"week\"]           = self.data['date'].dt.isocalendar().week\n        self.data[\"day\"]            = self.data['date'].dt.day\n        self.data[\"day_of_week\"]    = self.data['date'].dt.dayofweek\n        self.data[\"is_month_start\"] = self.data['date'].dt.is_month_start\n\n        self.data.drop('date', axis=1, inplace=True)\n\n        self.data = pd.get_dummies(data=self.data, columns=['holiday', 'seasons', 'functioning_day'])\n\n        self.next(self.load)\n\n    @step\n    def load(self):\n        import boto3\n        from os.path import join\n        from tempfile import TemporaryDirectory\n\n        self.interim = \"interim\"\n        self.clean_data_name = \"clean22.parquet\"\n\n        with TemporaryDirectory(\"hello_temp\") as tmp:\n            tmp_file = join(tmp, self.clean_data_name)\n            self.data.to_parquet(tmp_file)\n            s3_resource = boto3.resource(\"s3\", endpoint_url=\"http://localhost.localstack.cloud:4566\")         \n            s3_resource.Object(bucket_name=self.bucket, key=join(self.interim, self.clean_data_name)).upload_file(tmp_file)\n\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print(\"MainDataFlow has finished successfully!\")\n\nif __name__ == \"__main__\":\n    MainDataFlow()\n</code></pre> <p>Let's run our flow and then go over what just happened.</p> <pre><code>!python ../src/ml_eng/data_flow.py run\n</code></pre> <p>Everything metaflow flow has the following hierarchy.</p> <p>Metaflow &gt; Flow &gt; Run &gt; Step &gt; Task &gt; Artifact</p> <p>We can inspect what just our our flows via the the following commands.</p> <pre><code>from metaflow import Run\n</code></pre> <pre><code>from metaflow import Metaflow\nmf = Metaflow()\n</code></pre> <pre><code>print(mf.flows)\n</code></pre> <pre><code>from metaflow import Flow\nflow = Flow('FireFlow')\nruns = list(flow)\nruns\n</code></pre> <pre><code>runs[2]\n</code></pre> <pre><code>runs[2].data\n</code></pre> <pre><code>import pandas as pd\npd.read_parquet(runs[2].data.data_out).head()\n</code></pre> <p>We can also inspect the our bucket to see where the files we just worked with went.</p> <pre><code>!awslocal s3 ls datalake/\n</code></pre> <pre><code>!awslocal s3 ls datalake/raw/\n</code></pre> <p>We can also further examine each more of the characteristics of our last with the <code>current</code> function.</p> <pre><code>%%writefile ../src/ml_eng/current_flow.py\n\nfrom metaflow import FlowSpec, step, current\n\nclass CurrentFlow(FlowSpec):\n\n    @step\n    def start(self):\n        print(\"flow name: %s\" % current.flow_name)\n        print(\"run id: %s\" % current.run_id)\n        print(\"origin run id: %s\" % current.origin_run_id)\n        print(\"step name: %s\" % current.step_name)\n        print(\"task id: %s\" % current.task_id)\n        print(\"pathspec: %s\" % current.pathspec)\n        print(\"namespace: %s\" % current.namespace)\n        print(\"username: %s\" % current.username)\n        print(\"flow parameters: %s\" % str(current.parameter_names))\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print(\"end has a different step name: %s\" % current.step_name)\n        print(\"end has a different task id: %s\" % current.task_id)\n        print(\"end has a different pathspec: %s\" % current.pathspec)\n\nif __name__ == '__main__':\n    CurrentFlow()\n</code></pre> <pre><code>!python ../src/ml_eng/current_flow.py run\n</code></pre>"},{"location":"lessons/02_framework/#6-add-ons","title":"6. Add-Ons","text":"<p>One of the aspects that makes metaflow so powerful is the additional utilities it comes with.</p> <ul> <li>@resources --&gt; what resources our workflows need to use</li> <li>@conda --&gt; the environment we would like to use provided by conda</li> <li>@schedule --&gt; when to run our flows</li> <li>Parameter --&gt; what parameters can we share across the steps or via the command line</li> <li>cards --&gt; a way to visualize our flows</li> </ul> <pre><code>%%writefile ../src/ml_eng/better_flow.py\n\nimport urllib.request\nfrom pathlib import Path\nimport pandas as pd, re\n\nfrom metaflow import FlowSpec, step, Parameter, schedule, conda, \n\n\n@schedule(daily=True)\nclass BetterDataFlow(FlowSpec):\n\n\n    @step\n    def start(self):\n        import boto3\n        from os.path import join\n        import urllib.request\n\n        url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv'\n        self.bucket = \"datalake\"\n        self.raw_dir = \"raw\"\n        self.raw_data_name = 'SeoulBikeData_test.csv'\n        self.tmp_file, _ = urllib.request.urlretrieve(url, self.raw_data_name)\n\n        s3_resource = boto3.resource(\"s3\", endpoint_url=\"http://localhost.localstack.cloud:4566\")         \n        s3_resource.Object(bucket_name=self.bucket, key=join(self.raw_dir, self.raw_data_name)).upload_file(self.tmp_file)\n\n        self.next(self.extract)\n\n\n    @conda(python=\"3.10\", libraries={\"pandas\": \"1.5.3\"})\n    @step\n    def extract(self):\n        import pandas as pd\n\n        # get the data with its peculiar encoding\n        self.data = pd.read_csv(self.tmp_file, encoding='iso-8859-1', parse_dates=['Date'], infer_datetime_format=True)\n        self.next(self.transform)\n\n\n    @card\n    @step\n    def transform(self):\n        self.data.columns = [re.sub(r'[^a-zA-Z0-9\\s_]', '', col).lower().replace(r\" \", \"_\") for col in self.data.columns]\n\n        self.data.sort_values(['date', 'hour'], inplace=True)\n\n        self.data[\"year\"]           = self.data['date'].dt.year\n        self.data[\"month\"]          = self.data['date'].dt.month\n        self.data[\"week\"]           = self.data['date'].dt.isocalendar().week\n        self.data[\"day\"]            = self.data['date'].dt.day\n        self.data[\"day_of_week\"]    = self.data['date'].dt.dayofweek\n        self.data[\"is_month_start\"] = self.data['date'].dt.is_month_start\n\n        self.data.drop('date', axis=1, inplace=True)\n\n        self.data = pd.get_dummies(data=self.data, columns=['holiday', 'seasons', 'functioning_day'])\n\n        self.next(self.load)\n\n    @step\n    def load(self):\n        import boto3\n        from os.path import join\n        from tempfile import TemporaryDirectory\n\n        self.interim = \"interim\"\n        self.clean_data_name = \"clean22.parquet\"\n\n        with TemporaryDirectory(\"hello_temp\") as tmp:\n            tmp_file = join(tmp, self.clean_data_name)\n            self.data.to_parquet(tmp_file)\n            s3_resource = boto3.resource(\"s3\", endpoint_url=\"http://localhost.localstack.cloud:4566\")         \n            s3_resource.Object(bucket_name=self.bucket, key=join(self.interim, self.clean_data_name)).upload_file(tmp_file)\n\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print(\"MainDataFlow has finished successfully!\")\n\nif __name__ == \"__main__\":\n    BetterDataFlow()\n</code></pre>"},{"location":"lessons/02_framework/#9-data-observability","title":"9. Data Observability","text":"<pre><code>import pandas as pd\nimport whylogs as why\n\ndf = pd.read_parquet(\"../data/porto/porto.parquet\")\nresults = why.log(df)\n</code></pre> <pre><code>prof_view = results.view()\n</code></pre> <pre><code>from whylogs.viz import NotebookProfileVisualizer\n\nvisualization = NotebookProfileVisualizer()\nvisualization.set_profiles(target_profile_view=prof_view)\nvisualization.profile_summary()\n</code></pre>"}]}