{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Data Pipelines From Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ÄúWithout a systematic way to start and keep data clean, bad data will happen.‚Äù ~ Donato Diorio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flow](https://drawio-app.com/wp-content/uploads/2019/05/How-to-create-flow-charts-in-drawio-1-1.png)  \n",
    "Source: [draw.io](https://drawio-app.com/flowcharts/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Overview](##1.-Overview)\n",
    "2. [Learning Outcomes](##2.-Learning-Outcomes)\n",
    "3. [Data](##3.-Data)\n",
    "4. [Tools](##4.-Tools)\n",
    "5. [Data Pipelines](##5.-Data-Pipelines)\n",
    "6. [Building a Framework](##6.-Building-a-Framework)\n",
    "    - Set Up Dev Environment\n",
    "    - Extract\n",
    "    - Transform\n",
    "    - Load\n",
    "7. [Reproducible Pipelines](##7.-Reproducible-Pipelines)\n",
    "8. [Scheduling](##8.-Scheduling)\n",
    "9. [Exercises](##9.-Exercises)\n",
    "10. [Resources](##10.-Resources)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create successful and reproducible data pipelines we need -- at the bare minimum -- tools that allow us manage where and how we store our data, how we run our computations, and how we version control everything we do. This is what we will focus on in this part of the workshop.\n",
    "\n",
    "The assumption of this section is that most of your data work can fit in a computer but, if the need were to arise, you could still use the code in this section with a beefier machine and it would get the job done without a problem.\n",
    "\n",
    "Since the files we'll interact with will most likely live in a remote server, we'll \n",
    "1. extract a copy of the data we'll use and save it to our local files;\n",
    "2. transform the data into the shape and form we need it to be in;\n",
    "3. load it into a local data warehouse based on the popular tool, duckdb;\n",
    "4. version our code and data using dvc and git;\n",
    "5. create a command line tool to run all of our jobs;\n",
    "6. create a reproducible pipeline that can capture different workflows.\n",
    "\n",
    "Before we get started, let's go over the learning outcomes for today. :)\n",
    "\n",
    "Note: While one of the main components of data orchestration is scheduling, you can sill create pipelines and reproducible workflows that can be triggered manually rather than via a schedule."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Outcomes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, let's go over the learning outcomes for this section of the workshop.\n",
    "\n",
    "By the end of this lesson you will be able to,\n",
    "1. Discuss what ETL and ELT Pipelines are.\n",
    "2. Understand how to read and combine data that comes from different sources.\n",
    "3. Create data pipelines using open-source tools.\n",
    "4. Develop command line tools."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bikes](https://upload.wikimedia.org/wikipedia/commons/2/20/Bike_share.jpg)\n",
    "\n",
    "All three data files contain similar information about how many bicycles have been rented each hour, day, week and months for several years and for each city government we are working with.\n",
    "\n",
    "You can get more information about the data of each city using the following links.\n",
    "\n",
    "- [Seoul, South Korea](https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand#)\n",
    "- [London, England, UK](https://www.kaggle.com/hmavrodiev/london-bike-sharing-dataset)\n",
    "- [Washington, DC, USA](https://www.kaggle.com/marklvl/bike-sharing-dataset?select=hour.csv)\n",
    "- [Porto, Portugal](https://www.kaggle.com/datasets/imakash3011/rental-bike-sharing) -- This one was shared in Kaggle, but you can also find the original source with more, up-to-date data, [here](https://capitalbikeshare.com/system-data).\n",
    "\n",
    "Note: Some datasets might come with another file containing daily information, but for our purposes, we will be using the hourly one.\n",
    "\n",
    "Here are the variables that appear in all data sets.\n",
    "\n",
    "|     London      |             Seoul           |    Washington   |     Porto       |\n",
    "|:---------------:|:---------------------------:|:---------------:|:---------------:|\n",
    "| date            | Date                        | instant         |    instant      |\n",
    "| count           | Rented Bike Count           | date            |    dteday       |\n",
    "| temperature     | Hour                        | seasons         |    season       |\n",
    "| t2              | Temperature(C)              | year            |    yr           |\n",
    "| humidity        | Humidity(%)                 | month           |    mnth         |\n",
    "| wind_speed      | Wind speed (m/s)            | hour            |    hr           |\n",
    "| weather_code    | Visibility  (10m)           | is_holiday      |    holiday      |\n",
    "| is_holiday      | Dew point temperature(√Ø¬ø¬ΩC) | weekday         |    weekday      |\n",
    "| is_weekend      | Solar Radiation (MJ/m2)     | workingday      |    workingday   |\n",
    "| seasons         | Rainfall(mm)                | weathersit      |    weathersit   |\n",
    "|                 | Snowfall(cm)                | temperature     |    temp         |\n",
    "|                 | Seasons                     | count           |    atemp        |\n",
    "|                 | Holiday                     | humidity        |    hum          |\n",
    "|                 | Functioning Day             | wind_speed      |    windspeed    |\n",
    "|                 |                             | casual          |    casual       |\n",
    "|                 |                             | registered      |    registered   |\n",
    "|                 |                             |                 |    cnt          |\n",
    "\n",
    "Since all of these datasets were generated with different logic (e.g. celsius vs fahrenheit, or other divergent measures) and, most likely, by different systems, we can expect more inconsistencies than just unmatching column names, numerical formats, and data collected.\n",
    "\n",
    "We will walk through an example pipeline and several cleaning steps after we discuss the tools we will be using today."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools that we will use in this section of the workshop are the following.\n",
    "\n",
    "- [pandas](https://pandas.pydata.org/) - \"is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\"\n",
    "- [dvc](https://dvc.org/) - \"DVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code.\"\n",
    "- [ibis](https://ibis-project.org/) - \"Ibis is a Python library that provides a lightweight, universal interface for data wrangling. It helps Python users explore and transform data of any size, stored anywhere.\"\n",
    "- [typer](https://typer.tiangolo.com/) - \"Typer is a library for building CLI applications that users will love using and developers will love creating. Based on Python 3.6+ type hints.\"\n",
    "- [pathlib](https://docs.python.org/3/library/pathlib.html) - allows us to manipulate paths as if they were python objects.\n",
    "\n",
    "Let's get started building data pipelines! :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![etl_pipe](https://editor.analyticsvidhya.com/uploads/899101.png)  \n",
    "Source: [Striim](https://www.striim.com/)\n",
    "\n",
    "There are different kinds of data pipelines, but two, in particular, dominate a big part of the data engineering world today, ETL and ELT pipelines.\n",
    "\n",
    "**What are ETL Pipelines?**\n",
    "\n",
    "The acronym ETL stands for Extract, Transform, and Load, and it is the process where data gets extracted from one or multiple sources, it gets processed in-transit, and then it gets loaded into place where data consumers can use (e.g. a data warehouse). These consumers can be data analysts, data scientists, and machine learning engineers, among many others.\n",
    "\n",
    "**What are ELT Pipelines?**\n",
    "With this approach, all the data, structured and unstructured, gets loaded into a data lake or warehouse before it gets transformed. With this approach, a lot of money spent on compute can be saved by only processing the data we need rather than all of it.\n",
    " \n",
    " \n",
    "**What are Reverse ETL Pipelines?**\n",
    "Reverse ETL tools take data from the data lake or warehouse back into business (critical) applications. For example, information about new customers that have not yet been populated into salesforce or other marketing tools for further use by the marketing, sales, and finance teams, and so on...\n",
    "\n",
    "**Why should you learn how to create them?**\n",
    "\n",
    "Data Pipeline tools enable data integration strategies by allowing companies to gather data from multiple data sources and consolidate it into a single, centralized location. ETL tools also make it possible for different types of data to work together, for example, data generated by the company can be combined with GPS and Temperature data coming from different sources.\n",
    "\n",
    "As data professionals, our task is to create value for our organizations, our clients and our collaborators using some of or all the data at our disposal. However, there are factors that can delay this process a little bit or a lot, for example, we often need to understand beforehand,\n",
    "1. Information about the process by which the data we're dealing with was generated, e.g.\n",
    "    - Point of sale\n",
    "    - Clicks on an online marketplace like Amazon, Etzy, Ebay, ect.\n",
    "    - A/B Test Results\n",
    "    - ...\n",
    "2. Information about the transformations that occurred during the cleaning and merging process, prior to us jumping on board,\n",
    "    - Celsius degrees were converted into fahrenheit\n",
    "    - Prices in Chilean pesos were converted to Rands\n",
    "    - Non-numerical and unavailable observations now contain \"Not Available\" or \"Unknown\"\n",
    "    - ...\n",
    "3. Information about how the data was stored and where. For instance,\n",
    "    - Parquet format\n",
    "    - NOSQL or SQL database\n",
    "    - CSV\n",
    "    - ...\n",
    "\n",
    "Understanding how the three processes described above flow will help us have more knowledge about the data that we are going to use, and how to best access it, transform it, and model it before we put it to good use.\n",
    "\n",
    "Let's walk through an example of a data pipeline using data from wildfires between 1983-2020 in the United States. You can find more information about the dataset [here](https://www.kaggle.com/kkhandekar/total-wildfires-acres-affected-1983-2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path().cwd().parent/\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data_in = data_path.joinpath(\"example\", \"federal_firefighting_costs.csv\")\n",
    "pd.read_csv(example_data_in).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most columns contain a `$` dollar sign and some `,` commas, and because this forces Python to treat numbers as objects (or strings) rather than `int`'s or `float`'s, we will have to remove these signs in our transformation step after extracting the data and before loading a clean version of it to a new location. Let's create 3 re-usable functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw above, only the last 5 variables have commas (`,`) and dollar symbols (`$`) so we will replace both with an empty space (` \"\" `) using a `for` loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    for col in data.iloc[:, 1:].columns:\n",
    "        data[col] = data[col].str.replace(r'[^0-9]+', '', regex=True).astype(int)\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \"load\" process, we will save the data as a `parquet` file. This is one of the most popular formats to save data in due to its compression capabilities, orientation, and speed gains in analytical workloads.\n",
    "\n",
    "Here's an example on the differences between the row-like format and the columnar format of parquet files. If this interests you you can read more about it [here](https://datos.gob.es/en/blog/why-should-you-use-parquet-files-if-you-process-lot-data)\n",
    "\n",
    "![colvsrow](https://www.scylladb.com/wp-content/uploads/columnar-database-diagram.png)  \n",
    "Source: [Scylla DB](https://www.scylladb.com/glossary/columnar-database/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data, path):\n",
    "    data.to_parquet(path)\n",
    "    print(\"Successfully Loaded Your Modified Data!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an output path and with a file name to save our data as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data_out = data_path.joinpath(\"example\", \"my_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have all the steps ready, we create a new function containing our graph using the `flow` decorator. We can give this function a name, for example, `\"Example Pipeline! üòé\"` and then chain the tasks we created previously in the order in which they should be run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_etl(example_data_in, example_data_out):\n",
    "    data = extract(example_data_in)\n",
    "    data_clean = transform(data)\n",
    "    load(data_clean, example_data_out)\n",
    "    print(\"Your Pipeline Ran Successfully!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to run our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_etl(example_data_in, example_data_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure we did everything correctly, let's create a quick visualization with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(example_data_out).plot(\n",
    "    x='Year',\n",
    "    y=\"ForestService\", \n",
    "    kind='line',\n",
    "    title=\"Forest Service costs by year\"\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building a Framework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do our data engineering work with a personalized framework, there are a few strategies we would take.\n",
    "- We could keep the code in our computers and copy it to a new project whenever we need to create new pipelines.\n",
    "- We could create a package and either upload it to PyPI or [Gemfury](https://gemfury.com/) (a private repository of packages for different programming languages).\n",
    "- We could keep in neatly organized in GitHub and clone the repository to every new project.\n",
    "\n",
    "For this use case, it would be great to work with a library that we could continuously update rather than notebooks and files getting copied around. This will help us stay organized and manage dependencies more effectively.\n",
    "\n",
    "Let's get started. :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Setting Up a Dev Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building frameworks can be super straightforward or a slightly cumbersome project. Because of this and because we are already using an environment to run this workshop in, I will leave here what I think is an excellent resource for learning about how to create Python Packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\">\n",
       "    <iframe width=\"700\" height=\"450\"\n",
       "    src=\"https://youtube.com/embed/l7zS8Ld4_iA\"\n",
       "    </iframe>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<div align=\"center\">\n",
    "    <iframe width=\"700\" height=\"450\"\n",
    "    src=\"https://youtube.com/embed/l7zS8Ld4_iA\"\n",
    "    </iframe>\n",
    "</div>\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Extract"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting data might seem straightforward, but it can come with plenty of caveats. For example, to access sensitive data you might need not only credentials but also access to VPNs (virtual private networks), you might have similar data in different kinds of formats (customer tables stored in legacy databases and new ones), or you might have different data all stored in one place (images, tables, and text all in a datalake) -- if you're lucky.\n",
    "\n",
    "To tackle these challenges, (1) access and (2) distribution of data, companies such as [Airbyte](https://airbyte.com/), [Fivetran](https://www.fivetran.com/), and others, have come up with solutions that do all the heavy lifting for us. They have created tools that either give you connectors to common data sources (such as S3, BigQuery, and others), or give you an API so that you can develop a custom connector.\n",
    "\n",
    "That said, what we'll do in this section is to create functions that allow us to extract data in different formats or download a dataset from the web. We will create one for each of the examples that we have for the workshop today. Let's get started loading some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_load_file(url, path_out, file_name):\n",
    "    path = Path(path_out)\n",
    "    if not path.exists(): path.mkdir(parents=True)\n",
    "    urllib.request.urlretrieve(url, path.joinpath(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_and_load_file(\n",
    "    url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv\",\n",
    "    path_out=\"../data/example\",\n",
    "    file_name=\"seoul_exp.csv\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above will download the file into the `\"data/example\"` directory. If we do not specify a path for the file to `urllib.request.urlretrieve`, it would download the file into a temporary directory since it wouldn't know what to do with it or where to put it.\n",
    "\n",
    "A popular alternative to `urllib` is `wget`, so you can switch the tools in this function easily. The former is part of the Python Standard Library (so you'll never have to install it), and the latter can be installed with `pip` or `conda`.\n",
    "\n",
    "Let's create two more, one for csv files and the other for parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv(path_in, encoding=None):\n",
    "    return pd.read_csv(path_in, encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seoul_df = extract_from_csv(\n",
    "    path_in=data_path.joinpath(\"seoul\", \"raw\", \"SeoulBikeData.csv\")\n",
    ")\n",
    "seoul_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason, we've added `enconing` to our function is that files are not always shared in `utf-8` abd it is important to account for this discrepancy when creating an ETL framework. In fact, one of our datasets has a tricky format itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_parquet(path_in, **kwargs):\n",
    "    return pd.read_parquet(path_in, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_df = extract_from_parquet(\n",
    "    path_in=data_path.joinpath(\"porto\", \"bike_sharing_hourly.parquet\")\n",
    ")\n",
    "porto_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our files is stored in a SQLite database so we'll use the `sqlite3` module, which is part of of the [Python Standard Library](https://docs.python.org/3/library/index.html), to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_db(path_in, query):\n",
    "    conn = sqlite3.connect(path_in)\n",
    "    return pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_df = extract_from_db(\n",
    "    path_in=data_path.joinpath(\"london\", \"london_bikes.db\"),\n",
    "    query=\"SELECT * FROM uk_bikes\"\n",
    ")\n",
    "london_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, JSON files can be tricky to handle so we'll try two quick cases here using a try-except approach, but take note that as your projects evolve, it is highly likely that this function might change. A very good tool to handle large amounts of unstructured JSON that can later be formatted as a dataframe (or many), is [Dask Bags](https://examples.dask.org/bag.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_json(path_in, **kwargs):\n",
    "    try:\n",
    "        data =  pd.read_json(path_in, kwargs)\n",
    "    except:\n",
    "        with open(path_in, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "            data = pd.json_normalize(data, kwargs)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_df = extract_from_json(\n",
    "    path_in=data_path.joinpath(\"wash_dc\", \"washington.json\")\n",
    ")\n",
    "dc_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our functions, we want to create a file and attach to it the minimum functionality possible to use it as a command line tool. We will do this with the popular package called `typer`. It offers a delightful user experience, it respects (and annoys you at times) by type-checking your code, and it provides you with beautifully-formatted output based on the [rich](https://rich.readthedocs.io/en/latest/) python library.\n",
    "\n",
    "With `typer` we can create CLIs in several ways, and, for our purposes, we will pick the most straightforward one which works by adding `typer.run()` around a main function that encapsulates your application logic, in our case, our extract function and the upcoming ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/data_eng/extract.py\n",
    "\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import typer\n",
    "from typing import Optional\n",
    "from load import save_data\n",
    "\n",
    "\n",
    "def get_and_load_file(url, path_out, file_name):\n",
    "    path = Path(path_out)\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)\n",
    "    urllib.request.urlretrieve(url, path.joinpath(file_name))\n",
    "\n",
    "def extract_from_csv(path_in, encoding=None):\n",
    "    return pd.read_csv(path_in, encoding=encoding)\n",
    "\n",
    "def extract_from_db(path_in, query):\n",
    "    conn = sqlite3.connect(path_in)\n",
    "    return pd.read_sql_query(query, conn)\n",
    "\n",
    "def extract_from_parquet(path_in, **kwargs):\n",
    "    return pd.read_parquet(path_in, **kwargs)\n",
    "\n",
    "def extract_from_json(path_in, **kwargs):\n",
    "    try:\n",
    "        data =  pd.read_json(path_in, **kwargs)\n",
    "    except:\n",
    "        with open(path_in, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "            data = pd.json_normalize(data, kwargs)\n",
    "    return data\n",
    "\n",
    "def main(\n",
    "    arg: str = typer.Option(...),\n",
    "    url: Optional[str] = typer.Option(None),\n",
    "    path_in: Optional[str] = typer.Option(None),\n",
    "    path_out: Optional[str] = typer.Option(None),\n",
    "    encoding: Optional[str] = typer.Option(None),\n",
    "    file_name: Optional[str] = typer.Option(None),\n",
    "    query: Optional[str] = typer.Option(None),\n",
    "):\n",
    "    if arg == \"get\":\n",
    "        get_and_load_file(url, path_out, file_name)\n",
    "    elif arg == \"csv\":\n",
    "        data = extract_from_csv(path_in, encoding=encoding)\n",
    "        save_data(data, path_out, file_name)\n",
    "    elif arg == \"pq\":\n",
    "        data = extract_from_parquet(path_in)\n",
    "        save_data(data, path_out, file_name)\n",
    "    elif arg == \"json\":\n",
    "        data = extract_from_json(path_in)\n",
    "        save_data(data, path_out, file_name)\n",
    "    elif arg == \"db\":\n",
    "        data = extract_from_db(path_in, query)\n",
    "        save_data(data, path_out, file_name)\n",
    "    else:\n",
    "        print(f\"Could not understand argument {arg}. Please use 'pq' for parquet files, 'db' for database, json, or csv in lowercase.\")\n",
    "\n",
    "    print(\"Data Extracted Successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    typer.run(main)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we added different parameters to our `main` function to control the behavior of our CLI app.\n",
    "- `arg` - the kind of data we're extracting.\n",
    "- `url` - the url for when we need to download files from somewhere\n",
    "- `path_in` - where is the data at\n",
    "- `path_out` - where is that data going to\n",
    "- `encoding` - what kind of enconding are we reading the file with\n",
    "- `file_name` - what is going to be the new name of the output file\n",
    "- `query` - query for the data we want from the SQL database\n",
    "\n",
    "A few important things to note:\n",
    "- parameters containing underscores `_` will be switched into a dash `-` by `typer`, so `file_name` will be `file-name`\n",
    "- commands are run with two dashes and with a space in between it and the argument being passes, e.g. `--name PyCon`\n",
    "- `typer.Option(None)` indicates to `typer` that this command can be optional, hence, when we change the kind of file we're extracting we can by pass having to put an option on others.\n",
    "- by adding `Optional` around the type of a function parameter, you are also letting typer that that parameter is optional and that its default argument is `None`.\n",
    "- a parameter with only a type is a required parameter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Transform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"https://giphy.com/embed/R9zXHWAHyTjnq\" width=400 height=300 ></iframe>\n",
    "\n",
    "Depending on tasks that await the final output files or tables of an ETL pipeline (e.g. reporting metrics, building machine learning models, forecasting, etc.), this step can be very convoluted or slightly straightforward. In general, though, this step can include lots of cleaning and normalization functions, and a schema setter, among many others.\n",
    "\n",
    "The cleaning steps might include dealing with missing values, cleaning emails, names, addresses, and others, or putting values of similar nature into the same format (e.g. different weather measurements into one).\n",
    "\n",
    "Normalization can be anything from changing column names with the same values but different names to have the same one, or height in cm vs feet and inches to be in the same measure, or ..., you get the point. :)\n",
    "\n",
    "A schema is the way in which we represent not only the content and type of our new data files/tables, but also the way in which we represent their relationship. A common schema is the [star schema](https://www.databricks.com/glossary/star-schema). An uncommon (but rising star) schema is the [activity schema](https://www.activityschema.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create some toy data for our Bicycle problem\n",
    "\n",
    "toy_data = pd.DataFrame({\"Postal Codes\": [22345, 32442, 20007], \n",
    "                         \"Cities'\":       [\"Miami,  FL\", \"Dallas, TX\", \"Washington, DC\"],\n",
    "                         \"Dates as mm-dd-yyy\":         pd.date_range(start='9/27/2021', periods=3)})\n",
    "toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_col_names(index_of_cols: pd.Index):\n",
    "    return [re.sub(r'[^a-zA-Z0-9\\s_]', '', col).lower().replace(r\" \", \"_\") for col in index_of_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_col_names(toy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(data, date_col, hour_col=None):\n",
    "\n",
    "    data[\"date\"] = pd.to_datetime(data[date_col], infer_datetime_format=True)\n",
    "    if not data.columns.isin([\"hour\", \"Hour\", \"hr\", \"HR\"]).any():\n",
    "        data[\"hour\"] = data['date'].dt.hour\n",
    "        #Time series datasets need to be ordered by time.\n",
    "        data.sort_values([\"date\", \"hour\"], inplace=True)\n",
    "    elif hour_col:\n",
    "        data.sort_values([\"date\", hour_col], inplace=True)\n",
    "    else:\n",
    "        print(\"You must figure out how the hour works in your file.\")\n",
    "\n",
    "    data[\"year\"]           = data['date'].dt.year\n",
    "    data[\"month\"]          = data['date'].dt.month\n",
    "    data[\"week\"]           = data['date'].dt.isocalendar().week\n",
    "    data[\"day\"]            = data['date'].dt.day\n",
    "    data[\"day_of_week\"]    = data['date'].dt.dayofweek\n",
    "    data[\"is_month_start\"] = data['date'].dt.is_month_start    \n",
    "\n",
    "    data.drop('date', axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dates(london_df, \"timestamp\").head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common tasks in data science is to create dummy variables, which is ofter referred to as one-hot encoding. These are binary representations of a category, for example, if you have a columns with different kinds of cars, after you one-hot encode it, you will have one column for sedan, coupe, convertible, and so on, and each will be represented with a 0 or a 1 for when it is available and when it isn't, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(data, cols_list, **kwargs):\n",
    "    return pd.get_dummies(data=data, columns=cols_list, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot(seoul_df, [\"Seasons\", \"Holiday\"], drop_first=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_location_cols(data, place):\n",
    "    data[\"loc_id\"] = place\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_location_cols(dc_df, \"DC\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_and_drop(data, col_to_fix, mapping, cols_to_drop):\n",
    "    data[col_to_fix] = data[col_to_fix].map(mapping)\n",
    "    return data.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_london = {0: 'Spring', 1: 'Summer', 2: 'Fall', 3: 'Winter'}\n",
    "cols_drop_london = ['t2', 'weather_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_and_drop(london_df, \"seasonreal\", seasons_london, cols_drop_london).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_and_merge(data_lists):\n",
    "    pick_order = data_lists[0].columns #takes order of columns of the first dataset\n",
    "    #reindexing columns by the order of the first dataset, then sorting by date and hour.\n",
    "    new_list = [d.reindex(columns=pick_order).sort_values(['date', 'hour']) for d in data_lists]\n",
    "    return pd.concat(new_list) #merge all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick any two datasets and\n",
    "- select 5 columns from each\n",
    "- change column names as appropriate\n",
    "- use the order_and_nerge function to combine both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we take the same approach as with the extract set of functions and wrap our transform workflow into a `main` function, and then that `main` function into a typer CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/data_eng/transform.py\n",
    "\n",
    "import pandas as pd, re\n",
    "from pathlib import Path\n",
    "from extract import save_data\n",
    "import typer\n",
    "from typing import Optional, List\n",
    "\n",
    "def clean_col_names(index_of_cols: pd.Index):\n",
    "    return [re.sub(r'[^a-zA-Z0-9\\s_]', '', col).lower().replace(r\" \", \"_\") for col in index_of_cols]\n",
    "\n",
    "def extract_dates(data, date_col, hour_col=None):\n",
    "\n",
    "    data[\"date\"] = pd.to_datetime(data[date_col], infer_datetime_format=True)\n",
    "    if not data.columns.isin([\"hour\", \"Hour\", \"hr\", \"HR\"]).any():\n",
    "        data[\"hour\"] = data['date'].dt.hour\n",
    "        #Time series datasets need to be ordered by time.\n",
    "        data.sort_values([\"date\", \"hour\"], inplace=True)\n",
    "    elif hour_col:\n",
    "        data.sort_values([\"date\", hour_col], inplace=True)\n",
    "    else:\n",
    "        print(\"You must figure out how the hour works in your file.\")\n",
    "\n",
    "    data[\"year\"]           = data['date'].dt.year\n",
    "    data[\"month\"]          = data['date'].dt.month\n",
    "    data[\"week\"]           = data['date'].dt.isocalendar().week\n",
    "    data[\"day\"]            = data['date'].dt.day\n",
    "    data[\"day_of_week\"]    = data['date'].dt.dayofweek\n",
    "    data[\"is_month_start\"] = data['date'].dt.is_month_start    \n",
    "\n",
    "    data.drop('date', axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "def one_hot(data, cols_list):\n",
    "    return pd.get_dummies(data=data, columns=cols_list)\n",
    "\n",
    "def add_location_cols(data, place):\n",
    "    data[\"loc_id\"] = place\n",
    "    return data\n",
    "\n",
    "def fix_and_drop(data, col_to_fix, mapping, cols_to_drop):\n",
    "    data[col_to_fix] = data[col_to_fix].map(mapping)\n",
    "    return data.drop(cols_to_drop, axis=1)\n",
    "\n",
    "def order_and_merge(data_lists):\n",
    "    pick_order = data_lists[0].columns #takes order of columns of the first dataset\n",
    "    new_list = [d.reindex(columns=pick_order).sort_values(['date', 'hour']) for d in data_lists] #reindexing columns by the order of the first dataset, then sorting by date and hour.\n",
    "    return pd.concat(new_list) #merge all\n",
    "\n",
    "def main(\n",
    "    path_in:   str           = typer.Option(...),\n",
    "    date_col:  Optional[str] = typer.Option(None),\n",
    "    hour_col:  Optional[str] = typer.Option(None),\n",
    "    cols_list: List[str]     = typer.Option(None),\n",
    "    place:     Optional[str] = typer.Option(None),\n",
    "    path_out:  Optional[str] = typer.Option(None),\n",
    "    file_name: Optional[str] = typer.Option(None)\n",
    "):\n",
    "    data = pd.read_parquet(path_in)\n",
    "    # data.columns = clean_col_names(data.columns)\n",
    "    data = extract_dates(data, date_col, hour_col)\n",
    "    if cols_list:\n",
    "        data = one_hot(data, cols_list)\n",
    "    data.columns = clean_col_names(data.columns)\n",
    "    data = add_location_cols(data, place)\n",
    "    save_data(data, path_out, file_name)\n",
    "\n",
    "    print(\"Data Extracted Successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    typer.run(main)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loading stage, at least for analytical purposes, tends to be a data warehouse like BigQuery, Redshift, etc., but it can also be a directory in a data lake where files are saved in the highly optimized parquet format.\n",
    "\n",
    "In order for us to simulate a data warehouse locally, we will create a duckdb database using the python library `ibis`. The reason we will do it this ways is that ibis can translate the schema of our files into the duckdb SQL in one line of code. Effectively saving us boilerplate code that may or may not end up accounting for every feature in our dataframes.\n",
    "\n",
    "Ibis is a very cool project, especially if your SQL skills as basic as mine, so I highly encourage you to check it out. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(path_in, path_out, file_name, table_name):\n",
    "    path = Path(path_out)\n",
    "    conn = ibis.duckdb.connect(path.joinpath(file_name))\n",
    "    conn.register(path_in, table_name=table_name)\n",
    "    print(f\"Successfully loaded the {table_name} table!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_db(\n",
    "    path_in=data_path.joinpath(\"porto\", \"bike_sharing_hourly.parquet\"),\n",
    "    path_out=data_path.parent.joinpath(\"archive\"),\n",
    "    file_name=\"mytest_dw.ddb\",\n",
    "    table_name=\"one_day\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in our data to check that it was loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duck = duckdb.connect(\"../archive/mytest_dw.ddb\")\n",
    "duck.query(\"SELECT * FROM one_day\").df().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet(path_in, path_out, file_name):\n",
    "    path = Path(path_out)\n",
    "    pd.read_parquet(path_in).to_parquet(path.joinpath(file_name))\n",
    "    print(f\"Successfully loaded the {file_name} table!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, path_out, file_name):\n",
    "    path_out = Path(path_out)\n",
    "    if not path_out.exists(): path_out.mkdir(parents=True)\n",
    "    data.to_parquet(path_out.joinpath(file_name))\n",
    "    print(f\"Successfully loaded the {file_name} table!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the same steps as before and wrap our main function with `typer.run()` to make it a CLI, we test that it works well, and then go on to the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/data_eng/load.py\n",
    "\n",
    "from pathlib import Path\n",
    "import ibis\n",
    "import typer\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_db(path_in, path_out, file_name, table_name):\n",
    "    path = Path(path_out)\n",
    "    conn = ibis.duckdb.connect(path.joinpath(file_name))\n",
    "    conn.register(path_in, table_name=table_name)\n",
    "    print(f\"Successfully loaded the {table_name} table!\")\n",
    "\n",
    "def create_parquet(path_in, path_out, file_name):\n",
    "    path = Path(path_out)\n",
    "    pd.read_parquet(path_in).to_parquet(path.joinpath(file_name))\n",
    "    print(f\"Successfully loaded the {file_name} table!\")\n",
    "\n",
    "def save_data(data, path_out, file_name):\n",
    "    path_out = Path(path_out)\n",
    "    if not path_out.exists(): path_out.mkdir(parents=True)\n",
    "    data.to_parquet(path_out.joinpath(file_name))\n",
    "    print(f\"Successfully loaded the {file_name} table!\")\n",
    "\n",
    "\n",
    "def main(\n",
    "    kind: str = typer.Option(...),\n",
    "    path_in: Optional[str] = typer.Option(None), \n",
    "    path_out: Optional[str] = typer.Option(None),\n",
    "    file_name: Optional[str] = typer.Option(None),\n",
    "    table_name: Optional[str] = typer.Option(None)\n",
    "):\n",
    "    if kind == \"db\":\n",
    "        create_db(path_in, path_out, file_name, table_name)\n",
    "    elif kind == \"pq\":\n",
    "        create_parquet(path_in, path_out, file_name)\n",
    "    else:\n",
    "        print(f\"Could not understand argument {kind}. Please use 'pq' for parquet files or 'db' for database.\")\n",
    "\n",
    "    print(\"Data Extracted Successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    typer.run(main)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reproducible Pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Version Control or `dvc` is a tool for version almost any kind of file you can think of. From images, text, videos, and excel spreadsheets, to machine learning models and other artifacts. In addition, it is also an excellent tool for tracking machine learning experiments and for creating language agnostic pipelines where you can automatically version control the inputs and outputs of your workflows. \n",
    "\n",
    "If you have ever used git then the following commands will feel like home\n",
    "\n",
    "- `dvc init` --> this the first step to get started using dvc (after installing it of course)\n",
    "- `dvc remote add -d storage gdrive://your_hash` --> since we will be tracking files somewhere, this command will help set up a remote repository for these.\n",
    "- `dvc add` --> add a file to track.\n",
    "- `dvc push` --> push the file to your remote repository.\n",
    "- `dvc pull` --> pull the file from your remote repository and into your local machine.\n",
    "- `dvc stage` --> allows you to start adding steps to a pipeline. It will create a `dvc.yml` file containing the steps of the pipeline. This can be modified manually as well.\n",
    "- `dvc repro` --> Once we finish our stage, or as we add steps to it, we can run our pipeline with this command. The best part is that the steps will get cashed, so if nothing changes, nothing will get rerun.\n",
    "- `dvc dag` --> allows you to visualize your pipelines as a graph in the terminal.\n",
    "\n",
    "Finally, some of the settings for our repo will be available at `.dvc/config` and these can be changed manually at any time.\n",
    "\n",
    "Open a terminal in the main directory for this workshop and follow along."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our pipeline, we'll add the stages step by step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "dvc stage add --name seoul_extract \\\n",
    "    --deps data/seoul/SeoulBikeData.csv \\\n",
    "    --outs data/seoul/raw/seoul_raw.parquet \\\n",
    "    python src/data_eng/extract.py --arg csv \\\n",
    "        --path-in data/seoul/SeoulBikeData.csv \\\n",
    "        --path-out data/seoul/raw \\\n",
    "        --file-name seoul_raw.parquet --encoding iso-8859-1\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "dvc stage add --name seoul_transform \\\n",
    "    --deps data/seoul/raw/seoul_raw.parquet \\\n",
    "    --outs data/seoul/interim/seoul_clean.parquet \\\n",
    "    python src/data_eng/transform.py \\\n",
    "        --path-in data/seoul/seoul_raw.parquet \\\n",
    "        --date-col Date --hour-col Hour --place Seoul \\\n",
    "        --path-out data/seoul/interim --file-name seoul_clean.parquet\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the load stage into our pipeline following the examples above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "dvc stage add --name seoul_load \\\n",
    "    --deps data/seoul/interim/seoul_clean.parquet \\\n",
    "    --outs data/dwarehouse/analytics.db \\\n",
    "    python src/data_eng/load.py --kind db \\\n",
    "        --path-in data/seoul/interim/seoul_clean.parquet \\\n",
    "        --path-out data/dwarehouse --name analytics.db \\\n",
    "        --table-name seoul_main\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can run our pipeline, evaluate it, and push our files into our remote storage.\n",
    "\n",
    "```sh\n",
    "# 1\n",
    "dvc repro\n",
    "\n",
    "# 2\n",
    "dvc dag\n",
    "\n",
    "# 3\n",
    "dvc push\n",
    "```\n",
    "\n",
    "Afterwards, dvc will provide us with git files to track for our project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Scheduling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scheduling our workflows can be super convenient and time-savior when we know we need to run tasks repeatedly. A very common tool for the task is cron, and here is an excellent tutorial to get started with it.\n",
    "\n",
    "Note: Windows users would need to use Windows Subsystem for Linux to use cron or some other tool, potentially in PowerShell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\">\n",
       "    <iframe width=\"700\" height=\"450\"\n",
       "    src=\"https://www.youtube.com/embed/QZJ1drMQz1A\"\n",
       "    title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; \n",
       "    clipboard-write; encrypted-media; gyroscope; picture-in-picture; \n",
       "    web-share\" allowfullscreen>\n",
       "    </iframe>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<div align=\"center\">\n",
    "    <iframe width=\"700\" height=\"450\"\n",
    "    src=\"https://www.youtube.com/embed/QZJ1drMQz1A\"\n",
    "    title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; \n",
    "    clipboard-write; encrypted-media; gyroscope; picture-in-picture; \n",
    "    web-share\" allowfullscreen>\n",
    "    </iframe>\n",
    "</div>\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open the terminal, create a new directory named `dc_bikes`, and cd into it.\n",
    "2. Create two subdirectories, data and src.\n",
    "3. Create an ETL pipeline with two functions in the transform step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize a git and a dvc repository for the project in Exercise 1.\n",
    "2. Create a new repo in GitHub and commit your initial changes.\n",
    "3. Create a dvc pipeline with your three stages.\n",
    "4. Commit your changes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to expand your knowledge around the tools and concepts that we covered in this lesson, you might find the following list of resources helpful.\n",
    "\n",
    "- To learn more about pandas --> [Python for Data Analysis, 3E by Wes McKinney](https://wesmckinney.com/book/)\n",
    "- To learn more about ibis --> [Ibis + Substrait + DuckDB by Gil Forsyth](https://ibis-project.org/blog/ibis_substrait_to_duckdb/)\n",
    "- To learn more about duckdb --> [DuckDB Tutorial by Data with Marc](https://www.youtube.com/watch?v=AjsB6lM2-zw&ab_channel=DatawithMarc)\n",
    "- To learn more about dvc --> [ML Pipeline Decoupled: I managed to Write a Framework Agnostic ML Pipeline with DVC, Rust & Python](https://towardsdev.com/ml-pipeline-decoupled-i-managed-to-write-a-framework-agnostic-ml-pipeline-with-dvc-rust-python-287de68104c9)\n",
    "- To learn more about typer --> [Data and Machine Learning Model Versioning with DVC by Ruben Winastwan](https://towardsdatascience.com/data-and-machine-learning-model-versioning-with-dvc-34fdadd06b15)\n",
    "- To learn more about cron --> [Cron Job: A Comprehensive Guide for Beginners 2023 by Linas L.](https://www.hostinger.com/tutorials/cron-job)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63cacdd40c79cd85c1711e197200d577d287fc0f507a43a2fac2b1fee12c0e38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
