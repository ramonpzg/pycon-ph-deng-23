{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 A Framework Approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Automation is not the enemy of jobs. It frees up human beings to do higher-value work.\" ~ Andy Stern."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. Overview\n",
    "2. Learning Outcomes\n",
    "3. Tools\n",
    "4. The (Local) Cloud\n",
    "5. Classy ETLs\n",
    "6. Add-ons\n",
    "7. Data Validation\n",
    "7. Resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we want to supercharge the minimum requirements we established for ourselves in section 1 (i.e. storage, compute, and version control) while expanding into better portability and orchestration.\n",
    "\n",
    "To accomplish this, we'll give the driver seat to `metaflow`, a tool originally created at, and open-sourced by, Netflix a few years back.\n",
    "\n",
    "The tool now has a company, [Outerbounds](), as its main contributor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Outcomes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this session you will be able to,\n",
    "- create workflows using metaflow\n",
    "- schedule workflows with different time intervals\n",
    "- understand how to visually inspect workflows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools that we will use in this section of the workshop are the following.\n",
    "\n",
    "- [localstack](https://localstack.cloud/) --> \"LocalStack is a cloud service emulator that runs in a single container on your laptop or in your CI environment. With LocalStack, you can run your AWS applications or Lambdas entirely on your local machine without connecting to a remote cloud provider!\" ~ [localstack](localstack.cloud)\n",
    "- [Metaflow]() --> \"Metaflow is a human-friendly Python library that makes it straightforward to develop, deploy, and operate various kinds of data-intensive applications, in particular those involving data science and ML.\" ~ [Metaflow docs](https://docs.metaflow.org/introduction/what-is-metaflow)\n",
    "- [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) --> \"You use the AWS SDK for Python (Boto3) to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services.\" ~ [boto3 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "\n",
    "Let's get started by evaluating the (local) cloud. :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The (Local) Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LocalStack is a tool that allows us to emulate the cloud services provided by AWS in our local machines. Their free tier is a great to get started learning about the cloud and that's why we will be using it here.\n",
    "\n",
    "Note that `localstack` need docked to be installed in your machine so if this is not available, you won't be able to do a few of the steps below.\n",
    "\n",
    "That said, let's get started.\n",
    "\n",
    "\n",
    "Open up a new terminal start localstack with the following command.\n",
    "\n",
    "```sh\n",
    "localstack start\n",
    "```\n",
    "You should be able to see the following image.\n",
    "\n",
    "![lss](../images/localstack_start.png)\n",
    "\n",
    "Now that we have a \"Cloud\" instance running in our machines, let's start by creating a bucket in S3 using boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\", endpoint_url=\"http://localhost.localstack.cloud:4566\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because we are not actually interacting with a cloud provider like AWS, GCP or Azure, we need to point boto3 towards our local cloud using the parameter `endpoint_url`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.create_bucket(Bucket=\"datalake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.list_buckets()[\"Buckets\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have less verbose output, and more or less the same functionality, we can create resources instead of clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\"s3\", endpoint_url=\"http://localhost.localstack.cloud:4566\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource.create_bucket(Bucket=\"datalake2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bk in s3_resource.buckets.all():\n",
    "    print(bk.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to finish setting up localstack for our workshop, we'll need to set up the aws and the metaflow configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classy ETLs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way metaflow works is by having the user define classes that inherite the metaflow's `FlowSpec` class and represent a flow of whatever you'd like to do, e.g., a data pipeline, a dashboard, or training one or many machine learning models, among many other taks.\n",
    "\n",
    "The beauty of metaflow is its simplicity and customizable nature. It's downside is the lack of an easy-to-install and easy-to-set-up user interface where one could visually inspect ones flows.\n",
    "\n",
    "Let's get our hands dirty with our first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/ml_eng/fireflow.py\n",
    "\n",
    "from metaflow import FlowSpec, step\n",
    "\n",
    "class FireFlow(FlowSpec):\n",
    "    \n",
    "    @step\n",
    "    def start(self):\n",
    "        print(\"Hi, this is your first flow!\")\n",
    "        from pathlib import Path\n",
    "        self.data_path = Path().cwd().parent/\"data\"/\"example\"\n",
    "        self.data_in = self.data_path.joinpath(\"federal_firefighting_costs.csv\")\n",
    "        self.data_out = self.data_path.joinpath(\"fire_flow_output.parquet\")\n",
    "        self.next(self.extract)\n",
    "\n",
    "    @step\n",
    "    def extract(self):\n",
    "        import pandas as pd\n",
    "        self.data = pd.read_csv(self.data_in)\n",
    "        self.next(self.transform)\n",
    "\n",
    "    @step\n",
    "    def transform(self):\n",
    "        import pandas as pd\n",
    "        for col in self.data.iloc[:, 1:].columns:\n",
    "            self.data[col] = self.data[col].str.replace(r'[^0-9]+', '', regex=True).astype(int)\n",
    "        self.next(self.load)\n",
    "\n",
    "    @step\n",
    "    def load(self):\n",
    "        import pandas as pd\n",
    "        self.data.to_parquet(self.data_out)\n",
    "        self.next(self.end)\n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"Your first flow finished!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FireFlow()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our our file using the following command from our notebook.\n",
    "\n",
    "Note that the same command won't work from the parent directory as the files are referenced from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/ml_eng/fireflow.py run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go over what just happened.\n",
    "\n",
    "Metaflow keeps track of a lot of things when we run a flow, and the output it just gave us gives us context as to what is happening inside of it. Here's what each piece of it means.\n",
    "\n",
    "- `2023-02-26 10:57:24.693` --> Timestamp for the step\n",
    "- `1677380240122156` --> Run ID\n",
    "- `load` --> Step Name\n",
    "- `4` --> Task ID\n",
    "- `(pid 49081)` --> Process ID\n",
    "- `Task is starting.` --> Log Message\n",
    "\n",
    "The way metaflow passes arguments from one step to another and the way it keeps track of everything it touches is via the `self` argument. Each step is its own encapsulated container running in isolation but with awareness of where to go next after a step finishes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick one function of each of of the files from the last section (`extract.py`, `transform.py`, and `load.py`) and create a flow with metaflow. Name it, BikesFlow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll write a proper flow for our earlier pipeline and we'll keep improving it in the next session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/ml_eng/data_flow.py\n",
    "\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import pandas as pd, re\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter\n",
    "\n",
    "class MainDataFlow(FlowSpec):\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "        import boto3\n",
    "        from os.path import join\n",
    "        import urllib.request\n",
    "        \n",
    "        url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv'\n",
    "        self.bucket = \"datalake\"\n",
    "        self.raw_dir = \"raw\"\n",
    "        self.raw_data_name = 'SeoulBikeData_test.csv'\n",
    "        self.tmp_file, _ = urllib.request.urlretrieve(url, self.raw_data_name)\n",
    "\n",
    "        s3_resource = boto3.resource(\"s3\", endpoint_url=\"http://localhost.localstack.cloud:4566\")         \n",
    "        s3_resource.Object(bucket_name=self.bucket, key=join(self.raw_dir, self.raw_data_name)).upload_file(self.tmp_file)\n",
    "\n",
    "        self.next(self.extract)\n",
    "\n",
    "    @step\n",
    "    def extract(self):\n",
    "        import pandas as pd\n",
    "        \n",
    "        # get the data with its peculiar encoding\n",
    "        self.data = pd.read_csv(self.tmp_file, encoding='iso-8859-1', parse_dates=['Date'], infer_datetime_format=True)\n",
    "        self.next(self.transform)\n",
    "\n",
    "\n",
    "    @step\n",
    "    def transform(self):\n",
    "        self.data.columns = [re.sub(r'[^a-zA-Z0-9\\s_]', '', col).lower().replace(r\" \", \"_\") for col in self.data.columns]\n",
    "\n",
    "        self.data.sort_values(['date', 'hour'], inplace=True)\n",
    "\n",
    "        self.data[\"year\"]           = self.data['date'].dt.year\n",
    "        self.data[\"month\"]          = self.data['date'].dt.month\n",
    "        self.data[\"week\"]           = self.data['date'].dt.isocalendar().week\n",
    "        self.data[\"day\"]            = self.data['date'].dt.day\n",
    "        self.data[\"day_of_week\"]    = self.data['date'].dt.dayofweek\n",
    "        self.data[\"is_month_start\"] = self.data['date'].dt.is_month_start\n",
    "        \n",
    "        self.data.drop('date', axis=1, inplace=True)\n",
    "\n",
    "        self.data = pd.get_dummies(data=self.data, columns=['holiday', 'seasons', 'functioning_day'])\n",
    "\n",
    "        self.next(self.load)\n",
    "\n",
    "    @step\n",
    "    def load(self):\n",
    "        import boto3\n",
    "        from os.path import join\n",
    "        from tempfile import TemporaryDirectory\n",
    "\n",
    "        self.interim = \"interim\"\n",
    "        self.clean_data_name = \"clean22.parquet\"\n",
    "\n",
    "        with TemporaryDirectory(\"hello_temp\") as tmp:\n",
    "            tmp_file = join(tmp, self.clean_data_name)\n",
    "            self.data.to_parquet(tmp_file)\n",
    "            s3_resource = boto3.resource(\"s3\", endpoint_url=\"http://localhost.localstack.cloud:4566\")         \n",
    "            s3_resource.Object(bucket_name=self.bucket, key=join(self.interim, self.clean_data_name)).upload_file(tmp_file)\n",
    "        \n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"MainDataFlow has finished successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MainDataFlow()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our flow and then go over what just happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/ml_eng/data_flow.py run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything metaflow flow has the following hierarchy.\n",
    "\n",
    "> Metaflow > Flow > Run > Step > Task > Artifact\n",
    "\n",
    "We can inspect what just our our flows via the the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Metaflow\n",
    "mf = Metaflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mf.flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaflow import Flow\n",
    "flow = Flow('FireFlow')\n",
    "runs = list(flow)\n",
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs[2].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_parquet(runs[2].data.data_out).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the our bucket to see where the files we just worked with went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!awslocal s3 ls datalake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!awslocal s3 ls datalake/raw/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also further examine each more of the characteristics of our last with the `current` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/ml_eng/current_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, current\n",
    "\n",
    "class CurrentFlow(FlowSpec):\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "        print(\"flow name: %s\" % current.flow_name)\n",
    "        print(\"run id: %s\" % current.run_id)\n",
    "        print(\"origin run id: %s\" % current.origin_run_id)\n",
    "        print(\"step name: %s\" % current.step_name)\n",
    "        print(\"task id: %s\" % current.task_id)\n",
    "        print(\"pathspec: %s\" % current.pathspec)\n",
    "        print(\"namespace: %s\" % current.namespace)\n",
    "        print(\"username: %s\" % current.username)\n",
    "        print(\"flow parameters: %s\" % str(current.parameter_names))\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"end has a different step name: %s\" % current.step_name)\n",
    "        print(\"end has a different task id: %s\" % current.task_id)\n",
    "        print(\"end has a different pathspec: %s\" % current.pathspec)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    CurrentFlow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../src/ml_eng/current_flow.py run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Add-Ons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the aspects that makes metaflow so powerful is the additional utilities it comes with.\n",
    "\n",
    "- @resources --> what resources our workflows need to use\n",
    "- @conda --> the environment we would like to use provided by conda\n",
    "- @schedule --> when to run our flows\n",
    "- Parameter --> what parameters can we share across the steps or via the command line\n",
    "- cards --> a way to visualize our flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/ml_eng/better_flow.py\n",
    "\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import pandas as pd, re\n",
    "\n",
    "from metaflow import FlowSpec, step, Parameter, schedule, conda, \n",
    "\n",
    "\n",
    "@schedule(daily=True)\n",
    "class BetterDataFlow(FlowSpec):\n",
    "\n",
    "    \n",
    "    @step\n",
    "    def start(self):\n",
    "        import boto3\n",
    "        from os.path import join\n",
    "        import urllib.request\n",
    "        \n",
    "        url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv'\n",
    "        self.bucket = \"datalake\"\n",
    "        self.raw_dir = \"raw\"\n",
    "        self.raw_data_name = 'SeoulBikeData_test.csv'\n",
    "        self.tmp_file, _ = urllib.request.urlretrieve(url, self.raw_data_name)\n",
    "\n",
    "        s3_resource = boto3.resource(\"s3\", endpoint_url=\"http://localhost.localstack.cloud:4566\")         \n",
    "        s3_resource.Object(bucket_name=self.bucket, key=join(self.raw_dir, self.raw_data_name)).upload_file(self.tmp_file)\n",
    "\n",
    "        self.next(self.extract)\n",
    "\n",
    "\n",
    "    @conda(python=\"3.10\", libraries={\"pandas\": \"1.5.3\"})\n",
    "    @step\n",
    "    def extract(self):\n",
    "        import pandas as pd\n",
    "        \n",
    "        # get the data with its peculiar encoding\n",
    "        self.data = pd.read_csv(self.tmp_file, encoding='iso-8859-1', parse_dates=['Date'], infer_datetime_format=True)\n",
    "        self.next(self.transform)\n",
    "\n",
    "    \n",
    "    @card\n",
    "    @step\n",
    "    def transform(self):\n",
    "        self.data.columns = [re.sub(r'[^a-zA-Z0-9\\s_]', '', col).lower().replace(r\" \", \"_\") for col in self.data.columns]\n",
    "\n",
    "        self.data.sort_values(['date', 'hour'], inplace=True)\n",
    "\n",
    "        self.data[\"year\"]           = self.data['date'].dt.year\n",
    "        self.data[\"month\"]          = self.data['date'].dt.month\n",
    "        self.data[\"week\"]           = self.data['date'].dt.isocalendar().week\n",
    "        self.data[\"day\"]            = self.data['date'].dt.day\n",
    "        self.data[\"day_of_week\"]    = self.data['date'].dt.dayofweek\n",
    "        self.data[\"is_month_start\"] = self.data['date'].dt.is_month_start\n",
    "        \n",
    "        self.data.drop('date', axis=1, inplace=True)\n",
    "\n",
    "        self.data = pd.get_dummies(data=self.data, columns=['holiday', 'seasons', 'functioning_day'])\n",
    "\n",
    "        self.next(self.load)\n",
    "\n",
    "    @step\n",
    "    def load(self):\n",
    "        import boto3\n",
    "        from os.path import join\n",
    "        from tempfile import TemporaryDirectory\n",
    "\n",
    "        self.interim = \"interim\"\n",
    "        self.clean_data_name = \"clean22.parquet\"\n",
    "\n",
    "        with TemporaryDirectory(\"hello_temp\") as tmp:\n",
    "            tmp_file = join(tmp, self.clean_data_name)\n",
    "            self.data.to_parquet(tmp_file)\n",
    "            s3_resource = boto3.resource(\"s3\", endpoint_url=\"http://localhost.localstack.cloud:4566\")         \n",
    "            s3_resource.Object(bucket_name=self.bucket, key=join(self.interim, self.clean_data_name)).upload_file(tmp_file)\n",
    "        \n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        print(\"MainDataFlow has finished successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BetterDataFlow()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import whylogs as why\n",
    "\n",
    "df = pd.read_parquet(\"../data/porto/porto.parquet\")\n",
    "results = why.log(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_view = results.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.viz import NotebookProfileVisualizer\n",
    "\n",
    "visualization = NotebookProfileVisualizer()\n",
    "visualization.set_profiles(target_profile_view=prof_view)\n",
    "visualization.profile_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63cacdd40c79cd85c1711e197200d577d287fc0f507a43a2fac2b1fee12c0e38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
